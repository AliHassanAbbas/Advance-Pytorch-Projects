{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Research paper**:https://arxiv.org/pdf/1505.04597"
      ],
      "metadata": {
        "id": "u7jpijEp3u7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import cv2\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import random"
      ],
      "metadata": {
        "id": "lrWSkXjt_AG3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Readthis first**"
      ],
      "metadata": {
        "id": "4OTGd6kzRMe6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "first  download three files:(just check current  directory i ahve upload all these three files here)<br>\n",
        "1=**train-volume.tif**<br>\n",
        "2=**train-labels.tif**<br>\n",
        "3=**test-volume.tif**,<br>then make a folder name **ISBI2012**(in root directory),then add all these three files in that folder"
      ],
      "metadata": {
        "id": "JS9skPmGRObi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "012piTyOSoZ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Config**"
      ],
      "metadata": {
        "id": "HUzSHDdcAsv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mkdir(*paths):\n",
        "    for path in paths:\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path, exist_ok=True)\n",
        "\n",
        "ROOT_DIR = os.path.dirname(os.path.abspath('__file__'))\n",
        "DATA_DIR = os.path.join(ROOT_DIR, 'ISBI2012')\n",
        "CKPT_DIR = os.path.join(ROOT_DIR, 'checkpoints')\n",
        "LOG_DIR = os.path.join(ROOT_DIR, 'logs')\n",
        "TRAIN_LOG_DIR = os.path.join(LOG_DIR, 'train')\n",
        "VAL_LOG_DIR = os.path.join(LOG_DIR, 'val')\n",
        "\n",
        "IMGS_DIR = os.path.join(DATA_DIR, 'imgs')\n",
        "LABELS_DIR = os.path.join(DATA_DIR, 'labels')\n",
        "\n",
        "TRAIN_IMGS_DIR = os.path.join(IMGS_DIR, 'train')\n",
        "VAL_IMGS_DIR = os.path.join(IMGS_DIR, 'val')\n",
        "TEST_IMGS_DIR = os.path.join(IMGS_DIR, 'test')\n",
        "\n",
        "TRAIN_LABELS_DIR = os.path.join(LABELS_DIR, 'train')\n",
        "VAL_LABELS_DIR = os.path.join(LABELS_DIR, 'val')\n",
        "TEST_LABELS_DIR = os.path.join(LABELS_DIR, 'test')\n",
        "\n",
        "mkdir(\n",
        "    CKPT_DIR, LOG_DIR, TRAIN_LOG_DIR, VAL_LOG_DIR,\n",
        "    IMGS_DIR, LABELS_DIR, TRAIN_IMGS_DIR, VAL_IMGS_DIR, TEST_IMGS_DIR,\n",
        "    TRAIN_LABELS_DIR, VAL_LABELS_DIR, TEST_LABELS_DIR,\n",
        "    )\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper parameters\n",
        "class Config:\n",
        "    LEARNING_RATE = 1e-3\n",
        "    BATCH_SIZE = 2\n",
        "    NUM_EPOCHS = 100"
      ],
      "metadata": {
        "id": "n6hl0W6AAuqN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data preprocess**"
      ],
      "metadata": {
        "id": "QOOuKvY4Ig5U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Data**"
      ],
      "metadata": {
        "id": "vj00gge0A0lF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_img_path = os.path.join(DATA_DIR, 'train-volume.tif')\n",
        "train_label_path = os.path.join(DATA_DIR, 'train-labels.tif')\n",
        "\n",
        "train_imgs = Image.open(train_img_path)\n",
        "train_labels = Image.open(train_label_path)"
      ],
      "metadata": {
        "id": "BjyQ2kXMA2gl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split Files**"
      ],
      "metadata": {
        "id": "n9Deu2A7IUnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_list = list()\n",
        "\n",
        "for n in range(train_imgs.n_frames):\n",
        "    train_imgs.seek(n)\n",
        "    train_labels.seek(n)\n",
        "\n",
        "    train_img = np.asarray(train_imgs)\n",
        "    train_label = np.asarray(train_labels)\n",
        "\n",
        "    data_list.append((train_img, train_label))\n",
        "\n",
        "# Shuffle and Split Data\n",
        "random.shuffle(data_list)\n",
        "train_list = data_list[:24]\n",
        "val_list = data_list[24:27]\n",
        "test_list = data_list[27:]"
      ],
      "metadata": {
        "id": "1WHy95tYIHbt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save splitted files**"
      ],
      "metadata": {
        "id": "3W0Ol_ExIcpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, data in enumerate(train_list, 1):\n",
        "    img, label = data\n",
        "    img_dst = os.path.join(TRAIN_IMGS_DIR, f'{idx:02}.png')\n",
        "    label_dst = os.path.join(TRAIN_LABELS_DIR, f'{idx:02}.png')\n",
        "    cv2.imwrite(img_dst, img)\n",
        "    cv2.imwrite(label_dst, label)\n",
        "\n",
        "for idx, data in enumerate(val_list, 1):\n",
        "    img, label = data\n",
        "    img_dst = os.path.join(VAL_IMGS_DIR, f'{idx:02}.png')\n",
        "    label_dst = os.path.join(VAL_LABELS_DIR, f'{idx:02}.png')\n",
        "    cv2.imwrite(img_dst, img)\n",
        "    cv2.imwrite(label_dst, label)\n",
        "\n",
        "for idx, data in enumerate(test_list, 1):\n",
        "    img, label = data\n",
        "    img_dst = os.path.join(TEST_IMGS_DIR, f'{idx:02}.png')\n",
        "    label_dst = os.path.join(TEST_LABELS_DIR, f'{idx:02}.png')\n",
        "    cv2.imwrite(img_dst, img)\n",
        "    cv2.imwrite(label_dst, label)"
      ],
      "metadata": {
        "id": "r3oFLMeSIb-s"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset**"
      ],
      "metadata": {
        "id": "MoCG0AwJAdNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "__all__ = ['Dataset', 'ToTensor', 'GrayscaleNormalization', 'RandomFlip']"
      ],
      "metadata": {
        "id": "ef9jDmCbItPE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, imgs_dir, labels_dir, transform=None):\n",
        "        self.transform = transform\n",
        "        self.imgs = sorted(glob.glob(os.path.join(imgs_dir, '*.png')))\n",
        "        self.labels = sorted(glob.glob(os.path.join(labels_dir, '*.png')))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = cv2.imread(self.imgs[index], cv2.IMREAD_GRAYSCALE) / 255.\n",
        "        label = cv2.imread(self.labels[index], cv2.IMREAD_GRAYSCALE) / 255.\n",
        "\n",
        "        ret = {\n",
        "            'img': img[:, :, np.newaxis],\n",
        "            'label': label[:, :, np.newaxis],\n",
        "        }\n",
        "\n",
        "        if self.transform:\n",
        "            ret = self.transform(ret)\n",
        "\n",
        "        return ret"
      ],
      "metadata": {
        "id": "K7A-lDpSIvy7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ToTensor:\n",
        "    def __call__(self, data):\n",
        "        img, label = data['img'], data['label']\n",
        "\n",
        "        img = img.transpose(2, 0, 1).astype(np.float32)  # torch 의 경우 (C, H, W)\n",
        "        label = label.transpose(2, 0, 1).astype(np.float32)\n",
        "\n",
        "        ret = {\n",
        "            'img': torch.from_numpy(img),\n",
        "            'label': torch.from_numpy(label),\n",
        "        }\n",
        "        return ret"
      ],
      "metadata": {
        "id": "svmreCz-IyF8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrayscaleNormalization:\n",
        "    def __init__(self, mean=0.5, std=0.5):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, data):\n",
        "        img, label = data['img'], data['label']\n",
        "        img = (img - self.mean) / self.std\n",
        "\n",
        "        ret = {\n",
        "            'img': img,\n",
        "            'label': label,\n",
        "        }\n",
        "        return ret"
      ],
      "metadata": {
        "id": "THJJPXI0Iz30"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomFlip:\n",
        "    def __call__(self, data):\n",
        "        img, label = data['img'], data['label']\n",
        "\n",
        "        if np.random.rand() > 0.5:\n",
        "            img = np.fliplr(img)\n",
        "            label = np.fliplr(label)\n",
        "\n",
        "        if np.random.rand() > 0.5:\n",
        "            img = np.flipud(img)\n",
        "            label = np.flipud(label)\n",
        "\n",
        "        ret = {\n",
        "            'img': img,\n",
        "            'label': label,\n",
        "        }\n",
        "        return ret"
      ],
      "metadata": {
        "id": "tzIVCldDI118"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Utils**"
      ],
      "metadata": {
        "id": "5-Hr8ml1AfmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "__all__ = ['to_numpy', 'denormalization', 'classify_class', 'save_net', 'load_net']"
      ],
      "metadata": {
        "id": "BBgAv8OaJJTL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_numpy(tensor):\n",
        "    return tensor.to('cpu').detach().numpy().transpose(0, 2, 3, 1)  # (Batch, H, W, C)\n",
        "\n",
        "def denormalization(data, mean, std):\n",
        "    return (data * std) + mean\n",
        "\n",
        "def classify_class(x):\n",
        "    return 1.0 * (x > 0.5)\n",
        "\n",
        "def save_net(ckpt_dir, net, optim, epoch):\n",
        "    if not os.path.exists(ckpt_dir):\n",
        "        os.makedirs(ckpt_dir)\n",
        "\n",
        "    torch.save(\n",
        "        {'net': net.state_dict(),'optim': optim.state_dict()},\n",
        "        os.path.join(ckpt_dir, f'model_epoch{epoch:04}.pth'),\n",
        "    )\n",
        "\n",
        "def load_net(ckpt_dir, net, optim):\n",
        "    if not os.path.exists(ckpt_dir):\n",
        "        os.makedirs(ckpt_dir)\n",
        "\n",
        "    ckpt_list = os.listdir(ckpt_dir)\n",
        "    ckpt_list.sort(key=lambda fname: int(''.join(filter(str.isdigit, fname))))\n",
        "\n",
        "    ckpt_path = os.path.join(CKPT_DIR, ckpt_list[-1])\n",
        "    model_dict = torch.load(ckpt_path)\n",
        "    print(f'* Load {ckpt_path}')\n",
        "\n",
        "    net.load_state_dict(model_dict['net'])\n",
        "    optim.load_state_dict(model_dict['optim'])\n",
        "    epoch = int(''.join(filter(str.isdigit, ckpt_list[-1])))\n",
        "\n",
        "    return net, optim, epoch\n"
      ],
      "metadata": {
        "id": "baU12erO_AAP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MODEL**"
      ],
      "metadata": {
        "id": "5d8ZgNdO_9X-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        def CBR2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
        "                nn.BatchNorm2d(num_features=out_channels),\n",
        "                nn.ReLU(),\n",
        "            )\n",
        "\n",
        "        # Bottom-Up\n",
        "        self.enc1_1 = CBR2d(in_channels=1, out_channels=64)\n",
        "        self.enc1_2 = CBR2d(in_channels=64, out_channels=64)\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc2_1 = CBR2d(in_channels=64, out_channels=128)\n",
        "        self.enc2_2 = CBR2d(in_channels=128, out_channels=128)\n",
        "\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc3_1 = CBR2d(in_channels=128, out_channels=256)\n",
        "        self.enc3_2 = CBR2d(in_channels=256, out_channels=256)\n",
        "\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc4_1 = CBR2d(in_channels=256, out_channels=512)\n",
        "        self.enc4_2 = CBR2d(in_channels=512, out_channels=512)\n",
        "\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc5_1 = CBR2d(in_channels=512, out_channels=1024)\n",
        "\n",
        "        # Top-Down\n",
        "        self.dec5_1 = CBR2d(in_channels=1024, out_channels=512)\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(in_channels=512, out_channels=512, kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec4_2 = CBR2d(in_channels=2 * 512, out_channels=512)\n",
        "        self.dec4_1 = CBR2d(in_channels=512, out_channels=256)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(in_channels=256, out_channels=256, kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec3_2 = CBR2d(in_channels=2 * 256, out_channels=256)\n",
        "        self.dec3_1 = CBR2d(in_channels=256, out_channels=128)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec2_2 = CBR2d(in_channels=2 * 128, out_channels=128)\n",
        "        self.dec2_1 = CBR2d(in_channels=128, out_channels=64)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec1_2 = CBR2d(in_channels=2 * 64, out_channels=64)\n",
        "        self.dec1_1 = CBR2d(in_channels=64, out_channels=64)\n",
        "\n",
        "        # out_channels = category 갯수\n",
        "        self.conv1x1 = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1, stride=1, padding=0, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        enc1_1 = self.enc1_1(x)\n",
        "        enc1_2 = self.enc1_2(enc1_1)\n",
        "        pool1 = self.pool1(enc1_2)\n",
        "\n",
        "        enc2_1 = self.enc2_1(pool1)\n",
        "        enc2_2 = self.enc2_2(enc2_1)\n",
        "        pool2 = self.pool2(enc2_2)\n",
        "\n",
        "        enc3_1 = self.enc3_1(pool2)\n",
        "        enc3_2 = self.enc3_2(enc3_1)\n",
        "        pool3 = self.pool3(enc3_2)\n",
        "\n",
        "        enc4_1 = self.enc4_1(pool3)\n",
        "        enc4_2 = self.enc4_2(enc4_1)\n",
        "        pool4 = self.pool4(enc4_2)\n",
        "\n",
        "        enc5_1 = self.enc5_1(pool4)\n",
        "\n",
        "        # Decoder\n",
        "        dec5_1 = self.dec5_1(enc5_1)\n",
        "\n",
        "        upconv4 = self.upconv4(dec5_1)\n",
        "        cat4 = torch.cat((upconv4, enc4_2), dim=1)  # dim={0: batch, 1: channel, 2: height, 3: width}\n",
        "        dec4_2 = self.dec4_2(cat4)\n",
        "        dec4_1 = self.dec4_1(dec4_2)\n",
        "\n",
        "        upconv3 = self.upconv3(dec4_1)\n",
        "        cat3 = torch.cat((upconv3, enc3_2), dim=1)\n",
        "        dec3_2 = self.dec3_2(cat3)\n",
        "        dec3_1 = self.dec3_1(dec3_2)\n",
        "\n",
        "        upconv2 = self.upconv2(dec3_1)\n",
        "        cat2 = torch.cat((upconv2, enc2_2), dim=1)\n",
        "        dec2_2 = self.dec2_2(cat2)\n",
        "        dec2_1 = self.dec2_1(dec2_2)\n",
        "\n",
        "        upconv1 = self.upconv1(dec2_1)\n",
        "        cat1 = torch.cat((upconv1, enc1_2), dim=1)\n",
        "        dec1_2 = self.dec1_2(cat1)\n",
        "        dec1_1 = self.dec1_1(dec1_2)\n",
        "\n",
        "        x = self.conv1x1(dec1_1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "wcOwF814_xIW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "BYnkyjT__xaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = Config()\n",
        "train_transform = transforms.Compose([\n",
        "    GrayscaleNormalization(mean=0.5, std=0.5),\n",
        "    RandomFlip(),\n",
        "    ToTensor(),\n",
        "])\n",
        "val_transform = transforms.Compose([\n",
        "    GrayscaleNormalization(mean=0.5, std=0.5),\n",
        "    ToTensor(),\n",
        "])\n",
        "\n",
        "# Set Dataset\n",
        "train_dataset = Dataset(imgs_dir=TRAIN_IMGS_DIR, labels_dir=TRAIN_LABELS_DIR, transform=train_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "val_dataset = Dataset(imgs_dir=VAL_IMGS_DIR, labels_dir=VAL_LABELS_DIR, transform=val_transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=cfg.BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "train_data_num = len(train_dataset)\n",
        "val_data_num = len(val_dataset)\n",
        "\n",
        "train_batch_num = int(np.ceil(train_data_num / cfg.BATCH_SIZE)) # np.ceil 반올림\n",
        "val_batch_num = int(np.ceil(val_data_num / cfg.BATCH_SIZE))\n",
        "\n",
        "# Network\n",
        "net = UNet().to(device)\n",
        "\n",
        "# Loss Function\n",
        "loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "# Optimizer\n",
        "optim = torch.optim.Adam(params=net.parameters(), lr=cfg.LEARNING_RATE)\n",
        "\n",
        "# Tensorboard\n",
        "train_writer = SummaryWriter(log_dir=TRAIN_LOG_DIR)\n",
        "val_writer = SummaryWriter(log_dir=VAL_LOG_DIR)\n",
        "\n",
        "# Training\n",
        "start_epoch = 0\n",
        "# Load Checkpoint File\n",
        "if os.listdir(CKPT_DIR):\n",
        "    net, optim, start_epoch = load_net(ckpt_dir=CKPT_DIR, net=net, optim=optim)\n",
        "else:\n",
        "    print('* Training from scratch')\n",
        "\n",
        "num_epochs = cfg.NUM_EPOCHS\n",
        "for epoch in range(start_epoch+1, num_epochs+1):\n",
        "    net.train()  # Train Mode\n",
        "    train_loss_arr = list()\n",
        "\n",
        "    for batch_idx, data in enumerate(train_loader, 1):\n",
        "        # Forward Propagation\n",
        "        img = data['img'].to(device)\n",
        "        label = data['label'].to(device)\n",
        "\n",
        "        output = net(img)\n",
        "\n",
        "        # Backward Propagation\n",
        "        optim.zero_grad()\n",
        "\n",
        "        loss = loss_fn(output, label)\n",
        "        loss.backward()\n",
        "\n",
        "        optim.step()\n",
        "\n",
        "        # Calc Loss Function\n",
        "        train_loss_arr.append(loss.item())\n",
        "        print_form = '[Train] | Epoch: {:0>4d} / {:0>4d} | Batch: {:0>4d} / {:0>4d} | Loss: {:.4f}'\n",
        "        print(print_form.format(epoch, num_epochs, batch_idx, train_batch_num, train_loss_arr[-1]))\n",
        "\n",
        "        # Tensorboard\n",
        "        img = to_numpy(denormalization(img, mean=0.5, std=0.5))\n",
        "        label = to_numpy(label)\n",
        "        output = to_numpy(classify_class(output))\n",
        "\n",
        "        global_step = train_batch_num * (epoch-1) + batch_idx\n",
        "        train_writer.add_image(tag='img', img_tensor=img, global_step=global_step, dataformats='NHWC')\n",
        "        train_writer.add_image(tag='label', img_tensor=label, global_step=global_step, dataformats='NHWC')\n",
        "        train_writer.add_image(tag='output', img_tensor=output, global_step=global_step, dataformats='NHWC')\n",
        "\n",
        "    train_loss_avg = np.mean(train_loss_arr)\n",
        "    train_writer.add_scalar(tag='loss', scalar_value=train_loss_avg, global_step=epoch)\n",
        "\n",
        "    # Validation (No Back Propagation)\n",
        "    with torch.no_grad():\n",
        "        net.eval()  # Evaluation Mode\n",
        "        val_loss_arr = list()\n",
        "\n",
        "        for batch_idx, data in enumerate(val_loader, 1):\n",
        "            # Forward Propagation\n",
        "            img = data['img'].to(device)\n",
        "            label = data['label'].to(device)\n",
        "\n",
        "            output = net(img)\n",
        "\n",
        "            # Calc Loss Function\n",
        "            loss = loss_fn(output, label)\n",
        "            val_loss_arr.append(loss.item())\n",
        "\n",
        "            print_form = '[Validation] | Epoch: {:0>4d} / {:0>4d} | Batch: {:0>4d} / {:0>4d} | Loss: {:.4f}'\n",
        "            print(print_form.format(epoch, num_epochs, batch_idx, val_batch_num, val_loss_arr[-1]))\n",
        "\n",
        "            # Tensorboard\n",
        "            img = to_numpy(denormalization(img, mean=0.5, std=0.5))\n",
        "            label = to_numpy(label)\n",
        "            output = to_numpy(classify_class(output))\n",
        "\n",
        "            global_step = val_batch_num * (epoch-1) + batch_idx\n",
        "            val_writer.add_image(tag='img', img_tensor=img, global_step=global_step, dataformats='NHWC')\n",
        "            val_writer.add_image(tag='label', img_tensor=label, global_step=global_step, dataformats='NHWC')\n",
        "            val_writer.add_image(tag='output', img_tensor=output, global_step=global_step, dataformats='NHWC')\n",
        "\n",
        "    val_loss_avg = np.mean(val_loss_arr)\n",
        "    val_writer.add_scalar(tag='loss', scalar_value=val_loss_avg, global_step=epoch)\n",
        "\n",
        "    print_form = '[Epoch {:0>4d}] Training Avg Loss: {:.4f} | Validation Avg Loss: {:.4f}'\n",
        "    print(print_form.format(epoch, train_loss_avg, val_loss_avg))\n",
        "\n",
        "    save_net(ckpt_dir=CKPT_DIR, net=net, optim=optim, epoch=epoch)\n",
        "\n",
        "train_writer.close()\n",
        "val_writer.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7nMDVql-_s2",
        "outputId": "bd1d04ff-642f-4eb4-8431-09ca98b6bc08"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* Training from scratch\n",
            "[Train] | Epoch: 0001 / 0100 | Batch: 0001 / 0012 | Loss: 0.6964\n",
            "[Train] | Epoch: 0001 / 0100 | Batch: 0002 / 0012 | Loss: 0.5704\n",
            "[Train] | Epoch: 0001 / 0100 | Batch: 0003 / 0012 | Loss: 0.5351\n",
            "[Train] | Epoch: 0001 / 0100 | Batch: 0004 / 0012 | Loss: 0.4689\n",
            "[Train] | Epoch: 0001 / 0100 | Batch: 0005 / 0012 | Loss: 0.4519\n",
            "[Train] | Epoch: 0001 / 0100 | Batch: 0006 / 0012 | Loss: 0.4740\n",
            "[Train] | Epoch: 0001 / 0100 | Batch: 0007 / 0012 | Loss: 0.4378\n",
            "[Train] | Epoch: 0001 / 0100 | Batch: 0008 / 0012 | Loss: 0.4393\n",
            "[Train] | Epoch: 0001 / 0100 | Batch: 0009 / 0012 | Loss: 0.4187\n",
            "[Train] | Epoch: 0001 / 0100 | Batch: 0010 / 0012 | Loss: 0.4032\n",
            "[Train] | Epoch: 0001 / 0100 | Batch: 0011 / 0012 | Loss: 0.3817\n",
            "[Train] | Epoch: 0001 / 0100 | Batch: 0012 / 0012 | Loss: 0.3794\n",
            "[Validation] | Epoch: 0001 / 0100 | Batch: 0001 / 0002 | Loss: 0.5737\n",
            "[Validation] | Epoch: 0001 / 0100 | Batch: 0002 / 0002 | Loss: 0.5614\n",
            "[Epoch 0001] Training Avg Loss: 0.4714 | Validation Avg Loss: 0.5675\n",
            "[Train] | Epoch: 0002 / 0100 | Batch: 0001 / 0012 | Loss: 0.3749\n",
            "[Train] | Epoch: 0002 / 0100 | Batch: 0002 / 0012 | Loss: 0.3528\n",
            "[Train] | Epoch: 0002 / 0100 | Batch: 0003 / 0012 | Loss: 0.3432\n",
            "[Train] | Epoch: 0002 / 0100 | Batch: 0004 / 0012 | Loss: 0.3401\n",
            "[Train] | Epoch: 0002 / 0100 | Batch: 0005 / 0012 | Loss: 0.3443\n",
            "[Train] | Epoch: 0002 / 0100 | Batch: 0006 / 0012 | Loss: 0.3608\n",
            "[Train] | Epoch: 0002 / 0100 | Batch: 0007 / 0012 | Loss: 0.3317\n",
            "[Train] | Epoch: 0002 / 0100 | Batch: 0008 / 0012 | Loss: 0.3189\n",
            "[Train] | Epoch: 0002 / 0100 | Batch: 0009 / 0012 | Loss: 0.3125\n",
            "[Train] | Epoch: 0002 / 0100 | Batch: 0010 / 0012 | Loss: 0.3018\n",
            "[Train] | Epoch: 0002 / 0100 | Batch: 0011 / 0012 | Loss: 0.3212\n",
            "[Train] | Epoch: 0002 / 0100 | Batch: 0012 / 0012 | Loss: 0.3229\n",
            "[Validation] | Epoch: 0002 / 0100 | Batch: 0001 / 0002 | Loss: 0.6611\n",
            "[Validation] | Epoch: 0002 / 0100 | Batch: 0002 / 0002 | Loss: 0.5950\n",
            "[Epoch 0002] Training Avg Loss: 0.3354 | Validation Avg Loss: 0.6281\n",
            "[Train] | Epoch: 0003 / 0100 | Batch: 0001 / 0012 | Loss: 0.2883\n",
            "[Train] | Epoch: 0003 / 0100 | Batch: 0002 / 0012 | Loss: 0.2877\n",
            "[Train] | Epoch: 0003 / 0100 | Batch: 0003 / 0012 | Loss: 0.3026\n",
            "[Train] | Epoch: 0003 / 0100 | Batch: 0004 / 0012 | Loss: 0.3010\n",
            "[Train] | Epoch: 0003 / 0100 | Batch: 0005 / 0012 | Loss: 0.2755\n",
            "[Train] | Epoch: 0003 / 0100 | Batch: 0006 / 0012 | Loss: 0.2976\n",
            "[Train] | Epoch: 0003 / 0100 | Batch: 0007 / 0012 | Loss: 0.2758\n",
            "[Train] | Epoch: 0003 / 0100 | Batch: 0008 / 0012 | Loss: 0.3022\n",
            "[Train] | Epoch: 0003 / 0100 | Batch: 0009 / 0012 | Loss: 0.2759\n",
            "[Train] | Epoch: 0003 / 0100 | Batch: 0010 / 0012 | Loss: 0.2720\n",
            "[Train] | Epoch: 0003 / 0100 | Batch: 0011 / 0012 | Loss: 0.2688\n",
            "[Train] | Epoch: 0003 / 0100 | Batch: 0012 / 0012 | Loss: 0.2682\n",
            "[Validation] | Epoch: 0003 / 0100 | Batch: 0001 / 0002 | Loss: 0.3574\n",
            "[Validation] | Epoch: 0003 / 0100 | Batch: 0002 / 0002 | Loss: 0.3297\n",
            "[Epoch 0003] Training Avg Loss: 0.2847 | Validation Avg Loss: 0.3435\n",
            "[Train] | Epoch: 0004 / 0100 | Batch: 0001 / 0012 | Loss: 0.3068\n",
            "[Train] | Epoch: 0004 / 0100 | Batch: 0002 / 0012 | Loss: 0.2861\n",
            "[Train] | Epoch: 0004 / 0100 | Batch: 0003 / 0012 | Loss: 0.2546\n",
            "[Train] | Epoch: 0004 / 0100 | Batch: 0004 / 0012 | Loss: 0.2504\n",
            "[Train] | Epoch: 0004 / 0100 | Batch: 0005 / 0012 | Loss: 0.2536\n",
            "[Train] | Epoch: 0004 / 0100 | Batch: 0006 / 0012 | Loss: 0.2650\n",
            "[Train] | Epoch: 0004 / 0100 | Batch: 0007 / 0012 | Loss: 0.2527\n",
            "[Train] | Epoch: 0004 / 0100 | Batch: 0008 / 0012 | Loss: 0.2627\n",
            "[Train] | Epoch: 0004 / 0100 | Batch: 0009 / 0012 | Loss: 0.2416\n",
            "[Train] | Epoch: 0004 / 0100 | Batch: 0010 / 0012 | Loss: 0.2453\n",
            "[Train] | Epoch: 0004 / 0100 | Batch: 0011 / 0012 | Loss: 0.2569\n",
            "[Train] | Epoch: 0004 / 0100 | Batch: 0012 / 0012 | Loss: 0.2646\n",
            "[Validation] | Epoch: 0004 / 0100 | Batch: 0001 / 0002 | Loss: 0.3033\n",
            "[Validation] | Epoch: 0004 / 0100 | Batch: 0002 / 0002 | Loss: 0.2590\n",
            "[Epoch 0004] Training Avg Loss: 0.2617 | Validation Avg Loss: 0.2812\n",
            "[Train] | Epoch: 0005 / 0100 | Batch: 0001 / 0012 | Loss: 0.2471\n",
            "[Train] | Epoch: 0005 / 0100 | Batch: 0002 / 0012 | Loss: 0.2360\n",
            "[Train] | Epoch: 0005 / 0100 | Batch: 0003 / 0012 | Loss: 0.2750\n",
            "[Train] | Epoch: 0005 / 0100 | Batch: 0004 / 0012 | Loss: 0.2661\n",
            "[Train] | Epoch: 0005 / 0100 | Batch: 0005 / 0012 | Loss: 0.2563\n",
            "[Train] | Epoch: 0005 / 0100 | Batch: 0006 / 0012 | Loss: 0.2442\n",
            "[Train] | Epoch: 0005 / 0100 | Batch: 0007 / 0012 | Loss: 0.2507\n",
            "[Train] | Epoch: 0005 / 0100 | Batch: 0008 / 0012 | Loss: 0.2407\n",
            "[Train] | Epoch: 0005 / 0100 | Batch: 0009 / 0012 | Loss: 0.2420\n",
            "[Train] | Epoch: 0005 / 0100 | Batch: 0010 / 0012 | Loss: 0.2452\n",
            "[Train] | Epoch: 0005 / 0100 | Batch: 0011 / 0012 | Loss: 0.2301\n",
            "[Train] | Epoch: 0005 / 0100 | Batch: 0012 / 0012 | Loss: 0.2502\n",
            "[Validation] | Epoch: 0005 / 0100 | Batch: 0001 / 0002 | Loss: 0.2844\n",
            "[Validation] | Epoch: 0005 / 0100 | Batch: 0002 / 0002 | Loss: 0.2443\n",
            "[Epoch 0005] Training Avg Loss: 0.2486 | Validation Avg Loss: 0.2643\n",
            "[Train] | Epoch: 0006 / 0100 | Batch: 0001 / 0012 | Loss: 0.2503\n",
            "[Train] | Epoch: 0006 / 0100 | Batch: 0002 / 0012 | Loss: 0.2501\n",
            "[Train] | Epoch: 0006 / 0100 | Batch: 0003 / 0012 | Loss: 0.2305\n",
            "[Train] | Epoch: 0006 / 0100 | Batch: 0004 / 0012 | Loss: 0.2289\n",
            "[Train] | Epoch: 0006 / 0100 | Batch: 0005 / 0012 | Loss: 0.2280\n",
            "[Train] | Epoch: 0006 / 0100 | Batch: 0006 / 0012 | Loss: 0.2478\n",
            "[Train] | Epoch: 0006 / 0100 | Batch: 0007 / 0012 | Loss: 0.2335\n",
            "[Train] | Epoch: 0006 / 0100 | Batch: 0008 / 0012 | Loss: 0.2329\n",
            "[Train] | Epoch: 0006 / 0100 | Batch: 0009 / 0012 | Loss: 0.2383\n",
            "[Train] | Epoch: 0006 / 0100 | Batch: 0010 / 0012 | Loss: 0.2386\n",
            "[Train] | Epoch: 0006 / 0100 | Batch: 0011 / 0012 | Loss: 0.2270\n",
            "[Train] | Epoch: 0006 / 0100 | Batch: 0012 / 0012 | Loss: 0.2605\n",
            "[Validation] | Epoch: 0006 / 0100 | Batch: 0001 / 0002 | Loss: 0.2624\n",
            "[Validation] | Epoch: 0006 / 0100 | Batch: 0002 / 0002 | Loss: 0.2521\n",
            "[Epoch 0006] Training Avg Loss: 0.2389 | Validation Avg Loss: 0.2573\n",
            "[Train] | Epoch: 0007 / 0100 | Batch: 0001 / 0012 | Loss: 0.2279\n",
            "[Train] | Epoch: 0007 / 0100 | Batch: 0002 / 0012 | Loss: 0.2517\n",
            "[Train] | Epoch: 0007 / 0100 | Batch: 0003 / 0012 | Loss: 0.2286\n",
            "[Train] | Epoch: 0007 / 0100 | Batch: 0004 / 0012 | Loss: 0.2281\n",
            "[Train] | Epoch: 0007 / 0100 | Batch: 0005 / 0012 | Loss: 0.2307\n",
            "[Train] | Epoch: 0007 / 0100 | Batch: 0006 / 0012 | Loss: 0.2125\n",
            "[Train] | Epoch: 0007 / 0100 | Batch: 0007 / 0012 | Loss: 0.2213\n",
            "[Train] | Epoch: 0007 / 0100 | Batch: 0008 / 0012 | Loss: 0.2466\n",
            "[Train] | Epoch: 0007 / 0100 | Batch: 0009 / 0012 | Loss: 0.2073\n",
            "[Train] | Epoch: 0007 / 0100 | Batch: 0010 / 0012 | Loss: 0.2079\n",
            "[Train] | Epoch: 0007 / 0100 | Batch: 0011 / 0012 | Loss: 0.2258\n",
            "[Train] | Epoch: 0007 / 0100 | Batch: 0012 / 0012 | Loss: 0.2177\n",
            "[Validation] | Epoch: 0007 / 0100 | Batch: 0001 / 0002 | Loss: 0.2696\n",
            "[Validation] | Epoch: 0007 / 0100 | Batch: 0002 / 0002 | Loss: 0.2248\n",
            "[Epoch 0007] Training Avg Loss: 0.2255 | Validation Avg Loss: 0.2472\n",
            "[Train] | Epoch: 0008 / 0100 | Batch: 0001 / 0012 | Loss: 0.2136\n",
            "[Train] | Epoch: 0008 / 0100 | Batch: 0002 / 0012 | Loss: 0.2254\n",
            "[Train] | Epoch: 0008 / 0100 | Batch: 0003 / 0012 | Loss: 0.2187\n",
            "[Train] | Epoch: 0008 / 0100 | Batch: 0004 / 0012 | Loss: 0.2060\n",
            "[Train] | Epoch: 0008 / 0100 | Batch: 0005 / 0012 | Loss: 0.2100\n",
            "[Train] | Epoch: 0008 / 0100 | Batch: 0006 / 0012 | Loss: 0.2041\n",
            "[Train] | Epoch: 0008 / 0100 | Batch: 0007 / 0012 | Loss: 0.2135\n",
            "[Train] | Epoch: 0008 / 0100 | Batch: 0008 / 0012 | Loss: 0.2176\n",
            "[Train] | Epoch: 0008 / 0100 | Batch: 0009 / 0012 | Loss: 0.2207\n",
            "[Train] | Epoch: 0008 / 0100 | Batch: 0010 / 0012 | Loss: 0.2628\n",
            "[Train] | Epoch: 0008 / 0100 | Batch: 0011 / 0012 | Loss: 0.2198\n",
            "[Train] | Epoch: 0008 / 0100 | Batch: 0012 / 0012 | Loss: 0.2051\n",
            "[Validation] | Epoch: 0008 / 0100 | Batch: 0001 / 0002 | Loss: 0.2476\n",
            "[Validation] | Epoch: 0008 / 0100 | Batch: 0002 / 0002 | Loss: 0.2272\n",
            "[Epoch 0008] Training Avg Loss: 0.2181 | Validation Avg Loss: 0.2374\n",
            "[Train] | Epoch: 0009 / 0100 | Batch: 0001 / 0012 | Loss: 0.2198\n",
            "[Train] | Epoch: 0009 / 0100 | Batch: 0002 / 0012 | Loss: 0.2037\n",
            "[Train] | Epoch: 0009 / 0100 | Batch: 0003 / 0012 | Loss: 0.2133\n",
            "[Train] | Epoch: 0009 / 0100 | Batch: 0004 / 0012 | Loss: 0.2184\n",
            "[Train] | Epoch: 0009 / 0100 | Batch: 0005 / 0012 | Loss: 0.2366\n",
            "[Train] | Epoch: 0009 / 0100 | Batch: 0006 / 0012 | Loss: 0.2200\n",
            "[Train] | Epoch: 0009 / 0100 | Batch: 0007 / 0012 | Loss: 0.2015\n",
            "[Train] | Epoch: 0009 / 0100 | Batch: 0008 / 0012 | Loss: 0.2124\n",
            "[Train] | Epoch: 0009 / 0100 | Batch: 0009 / 0012 | Loss: 0.2030\n",
            "[Train] | Epoch: 0009 / 0100 | Batch: 0010 / 0012 | Loss: 0.2136\n",
            "[Train] | Epoch: 0009 / 0100 | Batch: 0011 / 0012 | Loss: 0.2466\n",
            "[Train] | Epoch: 0009 / 0100 | Batch: 0012 / 0012 | Loss: 0.1929\n",
            "[Validation] | Epoch: 0009 / 0100 | Batch: 0001 / 0002 | Loss: 0.2892\n",
            "[Validation] | Epoch: 0009 / 0100 | Batch: 0002 / 0002 | Loss: 0.2238\n",
            "[Epoch 0009] Training Avg Loss: 0.2151 | Validation Avg Loss: 0.2565\n",
            "[Train] | Epoch: 0010 / 0100 | Batch: 0001 / 0012 | Loss: 0.1998\n",
            "[Train] | Epoch: 0010 / 0100 | Batch: 0002 / 0012 | Loss: 0.2026\n",
            "[Train] | Epoch: 0010 / 0100 | Batch: 0003 / 0012 | Loss: 0.2127\n",
            "[Train] | Epoch: 0010 / 0100 | Batch: 0004 / 0012 | Loss: 0.2364\n",
            "[Train] | Epoch: 0010 / 0100 | Batch: 0005 / 0012 | Loss: 0.1962\n",
            "[Train] | Epoch: 0010 / 0100 | Batch: 0006 / 0012 | Loss: 0.2416\n",
            "[Train] | Epoch: 0010 / 0100 | Batch: 0007 / 0012 | Loss: 0.2189\n",
            "[Train] | Epoch: 0010 / 0100 | Batch: 0008 / 0012 | Loss: 0.2110\n",
            "[Train] | Epoch: 0010 / 0100 | Batch: 0009 / 0012 | Loss: 0.2115\n",
            "[Train] | Epoch: 0010 / 0100 | Batch: 0010 / 0012 | Loss: 0.2040\n",
            "[Train] | Epoch: 0010 / 0100 | Batch: 0011 / 0012 | Loss: 0.2192\n",
            "[Train] | Epoch: 0010 / 0100 | Batch: 0012 / 0012 | Loss: 0.1932\n",
            "[Validation] | Epoch: 0010 / 0100 | Batch: 0001 / 0002 | Loss: 0.2741\n",
            "[Validation] | Epoch: 0010 / 0100 | Batch: 0002 / 0002 | Loss: 0.2634\n",
            "[Epoch 0010] Training Avg Loss: 0.2122 | Validation Avg Loss: 0.2687\n",
            "[Train] | Epoch: 0011 / 0100 | Batch: 0001 / 0012 | Loss: 0.2208\n",
            "[Train] | Epoch: 0011 / 0100 | Batch: 0002 / 0012 | Loss: 0.2280\n",
            "[Train] | Epoch: 0011 / 0100 | Batch: 0003 / 0012 | Loss: 0.1903\n",
            "[Train] | Epoch: 0011 / 0100 | Batch: 0004 / 0012 | Loss: 0.2217\n",
            "[Train] | Epoch: 0011 / 0100 | Batch: 0005 / 0012 | Loss: 0.2045\n",
            "[Train] | Epoch: 0011 / 0100 | Batch: 0006 / 0012 | Loss: 0.2006\n",
            "[Train] | Epoch: 0011 / 0100 | Batch: 0007 / 0012 | Loss: 0.2044\n",
            "[Train] | Epoch: 0011 / 0100 | Batch: 0008 / 0012 | Loss: 0.2187\n",
            "[Train] | Epoch: 0011 / 0100 | Batch: 0009 / 0012 | Loss: 0.2310\n",
            "[Train] | Epoch: 0011 / 0100 | Batch: 0010 / 0012 | Loss: 0.2168\n",
            "[Train] | Epoch: 0011 / 0100 | Batch: 0011 / 0012 | Loss: 0.1818\n",
            "[Train] | Epoch: 0011 / 0100 | Batch: 0012 / 0012 | Loss: 0.2299\n",
            "[Validation] | Epoch: 0011 / 0100 | Batch: 0001 / 0002 | Loss: 0.2538\n",
            "[Validation] | Epoch: 0011 / 0100 | Batch: 0002 / 0002 | Loss: 0.2093\n",
            "[Epoch 0011] Training Avg Loss: 0.2124 | Validation Avg Loss: 0.2315\n",
            "[Train] | Epoch: 0012 / 0100 | Batch: 0001 / 0012 | Loss: 0.2099\n",
            "[Train] | Epoch: 0012 / 0100 | Batch: 0002 / 0012 | Loss: 0.1979\n",
            "[Train] | Epoch: 0012 / 0100 | Batch: 0003 / 0012 | Loss: 0.2043\n",
            "[Train] | Epoch: 0012 / 0100 | Batch: 0004 / 0012 | Loss: 0.2009\n",
            "[Train] | Epoch: 0012 / 0100 | Batch: 0005 / 0012 | Loss: 0.2001\n",
            "[Train] | Epoch: 0012 / 0100 | Batch: 0006 / 0012 | Loss: 0.1878\n",
            "[Train] | Epoch: 0012 / 0100 | Batch: 0007 / 0012 | Loss: 0.1777\n",
            "[Train] | Epoch: 0012 / 0100 | Batch: 0008 / 0012 | Loss: 0.2054\n",
            "[Train] | Epoch: 0012 / 0100 | Batch: 0009 / 0012 | Loss: 0.2311\n",
            "[Train] | Epoch: 0012 / 0100 | Batch: 0010 / 0012 | Loss: 0.2217\n",
            "[Train] | Epoch: 0012 / 0100 | Batch: 0011 / 0012 | Loss: 0.1753\n",
            "[Train] | Epoch: 0012 / 0100 | Batch: 0012 / 0012 | Loss: 0.2330\n",
            "[Validation] | Epoch: 0012 / 0100 | Batch: 0001 / 0002 | Loss: 0.2520\n",
            "[Validation] | Epoch: 0012 / 0100 | Batch: 0002 / 0002 | Loss: 0.2296\n",
            "[Epoch 0012] Training Avg Loss: 0.2038 | Validation Avg Loss: 0.2408\n",
            "[Train] | Epoch: 0013 / 0100 | Batch: 0001 / 0012 | Loss: 0.1982\n",
            "[Train] | Epoch: 0013 / 0100 | Batch: 0002 / 0012 | Loss: 0.1902\n",
            "[Train] | Epoch: 0013 / 0100 | Batch: 0003 / 0012 | Loss: 0.1961\n",
            "[Train] | Epoch: 0013 / 0100 | Batch: 0004 / 0012 | Loss: 0.2012\n",
            "[Train] | Epoch: 0013 / 0100 | Batch: 0005 / 0012 | Loss: 0.2016\n",
            "[Train] | Epoch: 0013 / 0100 | Batch: 0006 / 0012 | Loss: 0.2211\n",
            "[Train] | Epoch: 0013 / 0100 | Batch: 0007 / 0012 | Loss: 0.2070\n",
            "[Train] | Epoch: 0013 / 0100 | Batch: 0008 / 0012 | Loss: 0.2111\n",
            "[Train] | Epoch: 0013 / 0100 | Batch: 0009 / 0012 | Loss: 0.2540\n",
            "[Train] | Epoch: 0013 / 0100 | Batch: 0010 / 0012 | Loss: 0.2007\n",
            "[Train] | Epoch: 0013 / 0100 | Batch: 0011 / 0012 | Loss: 0.2125\n",
            "[Train] | Epoch: 0013 / 0100 | Batch: 0012 / 0012 | Loss: 0.2193\n",
            "[Validation] | Epoch: 0013 / 0100 | Batch: 0001 / 0002 | Loss: 0.2618\n",
            "[Validation] | Epoch: 0013 / 0100 | Batch: 0002 / 0002 | Loss: 0.2626\n",
            "[Epoch 0013] Training Avg Loss: 0.2094 | Validation Avg Loss: 0.2622\n",
            "[Train] | Epoch: 0014 / 0100 | Batch: 0001 / 0012 | Loss: 0.2102\n",
            "[Train] | Epoch: 0014 / 0100 | Batch: 0002 / 0012 | Loss: 0.2010\n",
            "[Train] | Epoch: 0014 / 0100 | Batch: 0003 / 0012 | Loss: 0.2018\n",
            "[Train] | Epoch: 0014 / 0100 | Batch: 0004 / 0012 | Loss: 0.1996\n",
            "[Train] | Epoch: 0014 / 0100 | Batch: 0005 / 0012 | Loss: 0.1839\n",
            "[Train] | Epoch: 0014 / 0100 | Batch: 0006 / 0012 | Loss: 0.2364\n",
            "[Train] | Epoch: 0014 / 0100 | Batch: 0007 / 0012 | Loss: 0.1907\n",
            "[Train] | Epoch: 0014 / 0100 | Batch: 0008 / 0012 | Loss: 0.2029\n",
            "[Train] | Epoch: 0014 / 0100 | Batch: 0009 / 0012 | Loss: 0.2434\n",
            "[Train] | Epoch: 0014 / 0100 | Batch: 0010 / 0012 | Loss: 0.1858\n",
            "[Train] | Epoch: 0014 / 0100 | Batch: 0011 / 0012 | Loss: 0.2007\n",
            "[Train] | Epoch: 0014 / 0100 | Batch: 0012 / 0012 | Loss: 0.1917\n",
            "[Validation] | Epoch: 0014 / 0100 | Batch: 0001 / 0002 | Loss: 0.2496\n",
            "[Validation] | Epoch: 0014 / 0100 | Batch: 0002 / 0002 | Loss: 0.2105\n",
            "[Epoch 0014] Training Avg Loss: 0.2040 | Validation Avg Loss: 0.2300\n",
            "[Train] | Epoch: 0015 / 0100 | Batch: 0001 / 0012 | Loss: 0.2155\n",
            "[Train] | Epoch: 0015 / 0100 | Batch: 0002 / 0012 | Loss: 0.1957\n",
            "[Train] | Epoch: 0015 / 0100 | Batch: 0003 / 0012 | Loss: 0.2002\n",
            "[Train] | Epoch: 0015 / 0100 | Batch: 0004 / 0012 | Loss: 0.1855\n",
            "[Train] | Epoch: 0015 / 0100 | Batch: 0005 / 0012 | Loss: 0.2436\n",
            "[Train] | Epoch: 0015 / 0100 | Batch: 0006 / 0012 | Loss: 0.1916\n",
            "[Train] | Epoch: 0015 / 0100 | Batch: 0007 / 0012 | Loss: 0.1913\n",
            "[Train] | Epoch: 0015 / 0100 | Batch: 0008 / 0012 | Loss: 0.1955\n",
            "[Train] | Epoch: 0015 / 0100 | Batch: 0009 / 0012 | Loss: 0.1962\n",
            "[Train] | Epoch: 0015 / 0100 | Batch: 0010 / 0012 | Loss: 0.1993\n",
            "[Train] | Epoch: 0015 / 0100 | Batch: 0011 / 0012 | Loss: 0.1886\n",
            "[Train] | Epoch: 0015 / 0100 | Batch: 0012 / 0012 | Loss: 0.1941\n",
            "[Validation] | Epoch: 0015 / 0100 | Batch: 0001 / 0002 | Loss: 0.2491\n",
            "[Validation] | Epoch: 0015 / 0100 | Batch: 0002 / 0002 | Loss: 0.2342\n",
            "[Epoch 0015] Training Avg Loss: 0.1998 | Validation Avg Loss: 0.2417\n",
            "[Train] | Epoch: 0016 / 0100 | Batch: 0001 / 0012 | Loss: 0.1854\n",
            "[Train] | Epoch: 0016 / 0100 | Batch: 0002 / 0012 | Loss: 0.2125\n",
            "[Train] | Epoch: 0016 / 0100 | Batch: 0003 / 0012 | Loss: 0.1958\n",
            "[Train] | Epoch: 0016 / 0100 | Batch: 0004 / 0012 | Loss: 0.1950\n",
            "[Train] | Epoch: 0016 / 0100 | Batch: 0005 / 0012 | Loss: 0.2196\n",
            "[Train] | Epoch: 0016 / 0100 | Batch: 0006 / 0012 | Loss: 0.2054\n",
            "[Train] | Epoch: 0016 / 0100 | Batch: 0007 / 0012 | Loss: 0.1952\n",
            "[Train] | Epoch: 0016 / 0100 | Batch: 0008 / 0012 | Loss: 0.1843\n",
            "[Train] | Epoch: 0016 / 0100 | Batch: 0009 / 0012 | Loss: 0.1847\n",
            "[Train] | Epoch: 0016 / 0100 | Batch: 0010 / 0012 | Loss: 0.1964\n",
            "[Train] | Epoch: 0016 / 0100 | Batch: 0011 / 0012 | Loss: 0.2130\n",
            "[Train] | Epoch: 0016 / 0100 | Batch: 0012 / 0012 | Loss: 0.2072\n",
            "[Validation] | Epoch: 0016 / 0100 | Batch: 0001 / 0002 | Loss: 0.2584\n",
            "[Validation] | Epoch: 0016 / 0100 | Batch: 0002 / 0002 | Loss: 0.2168\n",
            "[Epoch 0016] Training Avg Loss: 0.1995 | Validation Avg Loss: 0.2376\n",
            "[Train] | Epoch: 0017 / 0100 | Batch: 0001 / 0012 | Loss: 0.1905\n",
            "[Train] | Epoch: 0017 / 0100 | Batch: 0002 / 0012 | Loss: 0.1952\n",
            "[Train] | Epoch: 0017 / 0100 | Batch: 0003 / 0012 | Loss: 0.1976\n",
            "[Train] | Epoch: 0017 / 0100 | Batch: 0004 / 0012 | Loss: 0.1927\n",
            "[Train] | Epoch: 0017 / 0100 | Batch: 0005 / 0012 | Loss: 0.1828\n",
            "[Train] | Epoch: 0017 / 0100 | Batch: 0006 / 0012 | Loss: 0.1856\n",
            "[Train] | Epoch: 0017 / 0100 | Batch: 0007 / 0012 | Loss: 0.1673\n",
            "[Train] | Epoch: 0017 / 0100 | Batch: 0008 / 0012 | Loss: 0.2722\n",
            "[Train] | Epoch: 0017 / 0100 | Batch: 0009 / 0012 | Loss: 0.1809\n",
            "[Train] | Epoch: 0017 / 0100 | Batch: 0010 / 0012 | Loss: 0.1900\n",
            "[Train] | Epoch: 0017 / 0100 | Batch: 0011 / 0012 | Loss: 0.1850\n",
            "[Train] | Epoch: 0017 / 0100 | Batch: 0012 / 0012 | Loss: 0.1850\n",
            "[Validation] | Epoch: 0017 / 0100 | Batch: 0001 / 0002 | Loss: 0.2229\n",
            "[Validation] | Epoch: 0017 / 0100 | Batch: 0002 / 0002 | Loss: 0.1994\n",
            "[Epoch 0017] Training Avg Loss: 0.1937 | Validation Avg Loss: 0.2111\n",
            "[Train] | Epoch: 0018 / 0100 | Batch: 0001 / 0012 | Loss: 0.1760\n",
            "[Train] | Epoch: 0018 / 0100 | Batch: 0002 / 0012 | Loss: 0.1770\n",
            "[Train] | Epoch: 0018 / 0100 | Batch: 0003 / 0012 | Loss: 0.1927\n",
            "[Train] | Epoch: 0018 / 0100 | Batch: 0004 / 0012 | Loss: 0.1849\n",
            "[Train] | Epoch: 0018 / 0100 | Batch: 0005 / 0012 | Loss: 0.2099\n",
            "[Train] | Epoch: 0018 / 0100 | Batch: 0006 / 0012 | Loss: 0.1942\n",
            "[Train] | Epoch: 0018 / 0100 | Batch: 0007 / 0012 | Loss: 0.1790\n",
            "[Train] | Epoch: 0018 / 0100 | Batch: 0008 / 0012 | Loss: 0.1970\n",
            "[Train] | Epoch: 0018 / 0100 | Batch: 0009 / 0012 | Loss: 0.1829\n",
            "[Train] | Epoch: 0018 / 0100 | Batch: 0010 / 0012 | Loss: 0.2070\n",
            "[Train] | Epoch: 0018 / 0100 | Batch: 0011 / 0012 | Loss: 0.1870\n",
            "[Train] | Epoch: 0018 / 0100 | Batch: 0012 / 0012 | Loss: 0.1840\n",
            "[Validation] | Epoch: 0018 / 0100 | Batch: 0001 / 0002 | Loss: 0.2362\n",
            "[Validation] | Epoch: 0018 / 0100 | Batch: 0002 / 0002 | Loss: 0.2012\n",
            "[Epoch 0018] Training Avg Loss: 0.1893 | Validation Avg Loss: 0.2187\n",
            "[Train] | Epoch: 0019 / 0100 | Batch: 0001 / 0012 | Loss: 0.1869\n",
            "[Train] | Epoch: 0019 / 0100 | Batch: 0002 / 0012 | Loss: 0.1698\n",
            "[Train] | Epoch: 0019 / 0100 | Batch: 0003 / 0012 | Loss: 0.1971\n",
            "[Train] | Epoch: 0019 / 0100 | Batch: 0004 / 0012 | Loss: 0.1902\n",
            "[Train] | Epoch: 0019 / 0100 | Batch: 0005 / 0012 | Loss: 0.2094\n",
            "[Train] | Epoch: 0019 / 0100 | Batch: 0006 / 0012 | Loss: 0.1872\n",
            "[Train] | Epoch: 0019 / 0100 | Batch: 0007 / 0012 | Loss: 0.1838\n",
            "[Train] | Epoch: 0019 / 0100 | Batch: 0008 / 0012 | Loss: 0.1745\n",
            "[Train] | Epoch: 0019 / 0100 | Batch: 0009 / 0012 | Loss: 0.1745\n",
            "[Train] | Epoch: 0019 / 0100 | Batch: 0010 / 0012 | Loss: 0.1872\n",
            "[Train] | Epoch: 0019 / 0100 | Batch: 0011 / 0012 | Loss: 0.2124\n",
            "[Train] | Epoch: 0019 / 0100 | Batch: 0012 / 0012 | Loss: 0.1955\n",
            "[Validation] | Epoch: 0019 / 0100 | Batch: 0001 / 0002 | Loss: 0.2754\n",
            "[Validation] | Epoch: 0019 / 0100 | Batch: 0002 / 0002 | Loss: 0.2318\n",
            "[Epoch 0019] Training Avg Loss: 0.1890 | Validation Avg Loss: 0.2536\n",
            "[Train] | Epoch: 0020 / 0100 | Batch: 0001 / 0012 | Loss: 0.2090\n",
            "[Train] | Epoch: 0020 / 0100 | Batch: 0002 / 0012 | Loss: 0.2002\n",
            "[Train] | Epoch: 0020 / 0100 | Batch: 0003 / 0012 | Loss: 0.2012\n",
            "[Train] | Epoch: 0020 / 0100 | Batch: 0004 / 0012 | Loss: 0.1901\n",
            "[Train] | Epoch: 0020 / 0100 | Batch: 0005 / 0012 | Loss: 0.1758\n",
            "[Train] | Epoch: 0020 / 0100 | Batch: 0006 / 0012 | Loss: 0.1945\n",
            "[Train] | Epoch: 0020 / 0100 | Batch: 0007 / 0012 | Loss: 0.1827\n",
            "[Train] | Epoch: 0020 / 0100 | Batch: 0008 / 0012 | Loss: 0.1762\n",
            "[Train] | Epoch: 0020 / 0100 | Batch: 0009 / 0012 | Loss: 0.2034\n",
            "[Train] | Epoch: 0020 / 0100 | Batch: 0010 / 0012 | Loss: 0.1770\n",
            "[Train] | Epoch: 0020 / 0100 | Batch: 0011 / 0012 | Loss: 0.1859\n",
            "[Train] | Epoch: 0020 / 0100 | Batch: 0012 / 0012 | Loss: 0.1672\n",
            "[Validation] | Epoch: 0020 / 0100 | Batch: 0001 / 0002 | Loss: 0.2323\n",
            "[Validation] | Epoch: 0020 / 0100 | Batch: 0002 / 0002 | Loss: 0.1963\n",
            "[Epoch 0020] Training Avg Loss: 0.1886 | Validation Avg Loss: 0.2143\n",
            "[Train] | Epoch: 0021 / 0100 | Batch: 0001 / 0012 | Loss: 0.1950\n",
            "[Train] | Epoch: 0021 / 0100 | Batch: 0002 / 0012 | Loss: 0.1842\n",
            "[Train] | Epoch: 0021 / 0100 | Batch: 0003 / 0012 | Loss: 0.1702\n",
            "[Train] | Epoch: 0021 / 0100 | Batch: 0004 / 0012 | Loss: 0.2017\n",
            "[Train] | Epoch: 0021 / 0100 | Batch: 0005 / 0012 | Loss: 0.1752\n",
            "[Train] | Epoch: 0021 / 0100 | Batch: 0006 / 0012 | Loss: 0.1906\n",
            "[Train] | Epoch: 0021 / 0100 | Batch: 0007 / 0012 | Loss: 0.2022\n",
            "[Train] | Epoch: 0021 / 0100 | Batch: 0008 / 0012 | Loss: 0.1697\n",
            "[Train] | Epoch: 0021 / 0100 | Batch: 0009 / 0012 | Loss: 0.1866\n",
            "[Train] | Epoch: 0021 / 0100 | Batch: 0010 / 0012 | Loss: 0.2062\n",
            "[Train] | Epoch: 0021 / 0100 | Batch: 0011 / 0012 | Loss: 0.1929\n",
            "[Train] | Epoch: 0021 / 0100 | Batch: 0012 / 0012 | Loss: 0.1761\n",
            "[Validation] | Epoch: 0021 / 0100 | Batch: 0001 / 0002 | Loss: 0.2280\n",
            "[Validation] | Epoch: 0021 / 0100 | Batch: 0002 / 0002 | Loss: 0.2055\n",
            "[Epoch 0021] Training Avg Loss: 0.1876 | Validation Avg Loss: 0.2168\n",
            "[Train] | Epoch: 0022 / 0100 | Batch: 0001 / 0012 | Loss: 0.1717\n",
            "[Train] | Epoch: 0022 / 0100 | Batch: 0002 / 0012 | Loss: 0.1776\n",
            "[Train] | Epoch: 0022 / 0100 | Batch: 0003 / 0012 | Loss: 0.2031\n",
            "[Train] | Epoch: 0022 / 0100 | Batch: 0004 / 0012 | Loss: 0.1817\n",
            "[Train] | Epoch: 0022 / 0100 | Batch: 0005 / 0012 | Loss: 0.1782\n",
            "[Train] | Epoch: 0022 / 0100 | Batch: 0006 / 0012 | Loss: 0.1849\n",
            "[Train] | Epoch: 0022 / 0100 | Batch: 0007 / 0012 | Loss: 0.2039\n",
            "[Train] | Epoch: 0022 / 0100 | Batch: 0008 / 0012 | Loss: 0.1827\n",
            "[Train] | Epoch: 0022 / 0100 | Batch: 0009 / 0012 | Loss: 0.1903\n",
            "[Train] | Epoch: 0022 / 0100 | Batch: 0010 / 0012 | Loss: 0.1743\n",
            "[Train] | Epoch: 0022 / 0100 | Batch: 0011 / 0012 | Loss: 0.1870\n",
            "[Train] | Epoch: 0022 / 0100 | Batch: 0012 / 0012 | Loss: 0.1855\n",
            "[Validation] | Epoch: 0022 / 0100 | Batch: 0001 / 0002 | Loss: 0.2496\n",
            "[Validation] | Epoch: 0022 / 0100 | Batch: 0002 / 0002 | Loss: 0.1966\n",
            "[Epoch 0022] Training Avg Loss: 0.1851 | Validation Avg Loss: 0.2231\n",
            "[Train] | Epoch: 0023 / 0100 | Batch: 0001 / 0012 | Loss: 0.1734\n",
            "[Train] | Epoch: 0023 / 0100 | Batch: 0002 / 0012 | Loss: 0.1800\n",
            "[Train] | Epoch: 0023 / 0100 | Batch: 0003 / 0012 | Loss: 0.1782\n",
            "[Train] | Epoch: 0023 / 0100 | Batch: 0004 / 0012 | Loss: 0.2068\n",
            "[Train] | Epoch: 0023 / 0100 | Batch: 0005 / 0012 | Loss: 0.1641\n",
            "[Train] | Epoch: 0023 / 0100 | Batch: 0006 / 0012 | Loss: 0.1773\n",
            "[Train] | Epoch: 0023 / 0100 | Batch: 0007 / 0012 | Loss: 0.1777\n",
            "[Train] | Epoch: 0023 / 0100 | Batch: 0008 / 0012 | Loss: 0.1596\n",
            "[Train] | Epoch: 0023 / 0100 | Batch: 0009 / 0012 | Loss: 0.2171\n",
            "[Train] | Epoch: 0023 / 0100 | Batch: 0010 / 0012 | Loss: 0.1887\n",
            "[Train] | Epoch: 0023 / 0100 | Batch: 0011 / 0012 | Loss: 0.1955\n",
            "[Train] | Epoch: 0023 / 0100 | Batch: 0012 / 0012 | Loss: 0.1758\n",
            "[Validation] | Epoch: 0023 / 0100 | Batch: 0001 / 0002 | Loss: 0.2224\n",
            "[Validation] | Epoch: 0023 / 0100 | Batch: 0002 / 0002 | Loss: 0.2020\n",
            "[Epoch 0023] Training Avg Loss: 0.1828 | Validation Avg Loss: 0.2122\n",
            "[Train] | Epoch: 0024 / 0100 | Batch: 0001 / 0012 | Loss: 0.1770\n",
            "[Train] | Epoch: 0024 / 0100 | Batch: 0002 / 0012 | Loss: 0.1741\n",
            "[Train] | Epoch: 0024 / 0100 | Batch: 0003 / 0012 | Loss: 0.1827\n",
            "[Train] | Epoch: 0024 / 0100 | Batch: 0004 / 0012 | Loss: 0.1913\n",
            "[Train] | Epoch: 0024 / 0100 | Batch: 0005 / 0012 | Loss: 0.1972\n",
            "[Train] | Epoch: 0024 / 0100 | Batch: 0006 / 0012 | Loss: 0.1910\n",
            "[Train] | Epoch: 0024 / 0100 | Batch: 0007 / 0012 | Loss: 0.1775\n",
            "[Train] | Epoch: 0024 / 0100 | Batch: 0008 / 0012 | Loss: 0.1746\n",
            "[Train] | Epoch: 0024 / 0100 | Batch: 0009 / 0012 | Loss: 0.2074\n",
            "[Train] | Epoch: 0024 / 0100 | Batch: 0010 / 0012 | Loss: 0.1619\n",
            "[Train] | Epoch: 0024 / 0100 | Batch: 0011 / 0012 | Loss: 0.1664\n",
            "[Train] | Epoch: 0024 / 0100 | Batch: 0012 / 0012 | Loss: 0.1976\n",
            "[Validation] | Epoch: 0024 / 0100 | Batch: 0001 / 0002 | Loss: 0.2296\n",
            "[Validation] | Epoch: 0024 / 0100 | Batch: 0002 / 0002 | Loss: 0.2087\n",
            "[Epoch 0024] Training Avg Loss: 0.1832 | Validation Avg Loss: 0.2192\n",
            "[Train] | Epoch: 0025 / 0100 | Batch: 0001 / 0012 | Loss: 0.1938\n",
            "[Train] | Epoch: 0025 / 0100 | Batch: 0002 / 0012 | Loss: 0.2056\n",
            "[Train] | Epoch: 0025 / 0100 | Batch: 0003 / 0012 | Loss: 0.1829\n",
            "[Train] | Epoch: 0025 / 0100 | Batch: 0004 / 0012 | Loss: 0.1831\n",
            "[Train] | Epoch: 0025 / 0100 | Batch: 0005 / 0012 | Loss: 0.1817\n",
            "[Train] | Epoch: 0025 / 0100 | Batch: 0006 / 0012 | Loss: 0.2006\n",
            "[Train] | Epoch: 0025 / 0100 | Batch: 0007 / 0012 | Loss: 0.1726\n",
            "[Train] | Epoch: 0025 / 0100 | Batch: 0008 / 0012 | Loss: 0.1668\n",
            "[Train] | Epoch: 0025 / 0100 | Batch: 0009 / 0012 | Loss: 0.1717\n",
            "[Train] | Epoch: 0025 / 0100 | Batch: 0010 / 0012 | Loss: 0.1965\n",
            "[Train] | Epoch: 0025 / 0100 | Batch: 0011 / 0012 | Loss: 0.1807\n",
            "[Train] | Epoch: 0025 / 0100 | Batch: 0012 / 0012 | Loss: 0.1641\n",
            "[Validation] | Epoch: 0025 / 0100 | Batch: 0001 / 0002 | Loss: 0.2634\n",
            "[Validation] | Epoch: 0025 / 0100 | Batch: 0002 / 0002 | Loss: 0.2049\n",
            "[Epoch 0025] Training Avg Loss: 0.1833 | Validation Avg Loss: 0.2341\n",
            "[Train] | Epoch: 0026 / 0100 | Batch: 0001 / 0012 | Loss: 0.1554\n",
            "[Train] | Epoch: 0026 / 0100 | Batch: 0002 / 0012 | Loss: 0.2068\n",
            "[Train] | Epoch: 0026 / 0100 | Batch: 0003 / 0012 | Loss: 0.1780\n",
            "[Train] | Epoch: 0026 / 0100 | Batch: 0004 / 0012 | Loss: 0.1866\n",
            "[Train] | Epoch: 0026 / 0100 | Batch: 0005 / 0012 | Loss: 0.1739\n",
            "[Train] | Epoch: 0026 / 0100 | Batch: 0006 / 0012 | Loss: 0.1788\n",
            "[Train] | Epoch: 0026 / 0100 | Batch: 0007 / 0012 | Loss: 0.2049\n",
            "[Train] | Epoch: 0026 / 0100 | Batch: 0008 / 0012 | Loss: 0.1731\n",
            "[Train] | Epoch: 0026 / 0100 | Batch: 0009 / 0012 | Loss: 0.1615\n",
            "[Train] | Epoch: 0026 / 0100 | Batch: 0010 / 0012 | Loss: 0.1822\n",
            "[Train] | Epoch: 0026 / 0100 | Batch: 0011 / 0012 | Loss: 0.1905\n",
            "[Train] | Epoch: 0026 / 0100 | Batch: 0012 / 0012 | Loss: 0.1802\n",
            "[Validation] | Epoch: 0026 / 0100 | Batch: 0001 / 0002 | Loss: 0.2340\n",
            "[Validation] | Epoch: 0026 / 0100 | Batch: 0002 / 0002 | Loss: 0.1968\n",
            "[Epoch 0026] Training Avg Loss: 0.1810 | Validation Avg Loss: 0.2154\n",
            "[Train] | Epoch: 0027 / 0100 | Batch: 0001 / 0012 | Loss: 0.1719\n",
            "[Train] | Epoch: 0027 / 0100 | Batch: 0002 / 0012 | Loss: 0.1834\n",
            "[Train] | Epoch: 0027 / 0100 | Batch: 0003 / 0012 | Loss: 0.1954\n",
            "[Train] | Epoch: 0027 / 0100 | Batch: 0004 / 0012 | Loss: 0.1832\n",
            "[Train] | Epoch: 0027 / 0100 | Batch: 0005 / 0012 | Loss: 0.1742\n",
            "[Train] | Epoch: 0027 / 0100 | Batch: 0006 / 0012 | Loss: 0.1917\n",
            "[Train] | Epoch: 0027 / 0100 | Batch: 0007 / 0012 | Loss: 0.1729\n",
            "[Train] | Epoch: 0027 / 0100 | Batch: 0008 / 0012 | Loss: 0.1612\n",
            "[Train] | Epoch: 0027 / 0100 | Batch: 0009 / 0012 | Loss: 0.2083\n",
            "[Train] | Epoch: 0027 / 0100 | Batch: 0010 / 0012 | Loss: 0.1620\n",
            "[Train] | Epoch: 0027 / 0100 | Batch: 0011 / 0012 | Loss: 0.1759\n",
            "[Train] | Epoch: 0027 / 0100 | Batch: 0012 / 0012 | Loss: 0.1921\n",
            "[Validation] | Epoch: 0027 / 0100 | Batch: 0001 / 0002 | Loss: 0.2483\n",
            "[Validation] | Epoch: 0027 / 0100 | Batch: 0002 / 0002 | Loss: 0.1958\n",
            "[Epoch 0027] Training Avg Loss: 0.1810 | Validation Avg Loss: 0.2220\n",
            "[Train] | Epoch: 0028 / 0100 | Batch: 0001 / 0012 | Loss: 0.1808\n",
            "[Train] | Epoch: 0028 / 0100 | Batch: 0002 / 0012 | Loss: 0.1982\n",
            "[Train] | Epoch: 0028 / 0100 | Batch: 0003 / 0012 | Loss: 0.2105\n",
            "[Train] | Epoch: 0028 / 0100 | Batch: 0004 / 0012 | Loss: 0.1880\n",
            "[Train] | Epoch: 0028 / 0100 | Batch: 0005 / 0012 | Loss: 0.1662\n",
            "[Train] | Epoch: 0028 / 0100 | Batch: 0006 / 0012 | Loss: 0.1807\n",
            "[Train] | Epoch: 0028 / 0100 | Batch: 0007 / 0012 | Loss: 0.1722\n",
            "[Train] | Epoch: 0028 / 0100 | Batch: 0008 / 0012 | Loss: 0.1705\n",
            "[Train] | Epoch: 0028 / 0100 | Batch: 0009 / 0012 | Loss: 0.1686\n",
            "[Train] | Epoch: 0028 / 0100 | Batch: 0010 / 0012 | Loss: 0.1957\n",
            "[Train] | Epoch: 0028 / 0100 | Batch: 0011 / 0012 | Loss: 0.1943\n",
            "[Train] | Epoch: 0028 / 0100 | Batch: 0012 / 0012 | Loss: 0.1664\n",
            "[Validation] | Epoch: 0028 / 0100 | Batch: 0001 / 0002 | Loss: 0.2336\n",
            "[Validation] | Epoch: 0028 / 0100 | Batch: 0002 / 0002 | Loss: 0.1999\n",
            "[Epoch 0028] Training Avg Loss: 0.1827 | Validation Avg Loss: 0.2168\n",
            "[Train] | Epoch: 0029 / 0100 | Batch: 0001 / 0012 | Loss: 0.1808\n",
            "[Train] | Epoch: 0029 / 0100 | Batch: 0002 / 0012 | Loss: 0.1758\n",
            "[Train] | Epoch: 0029 / 0100 | Batch: 0003 / 0012 | Loss: 0.2092\n",
            "[Train] | Epoch: 0029 / 0100 | Batch: 0004 / 0012 | Loss: 0.1733\n",
            "[Train] | Epoch: 0029 / 0100 | Batch: 0005 / 0012 | Loss: 0.1692\n",
            "[Train] | Epoch: 0029 / 0100 | Batch: 0006 / 0012 | Loss: 0.1858\n",
            "[Train] | Epoch: 0029 / 0100 | Batch: 0007 / 0012 | Loss: 0.1638\n",
            "[Train] | Epoch: 0029 / 0100 | Batch: 0008 / 0012 | Loss: 0.1592\n",
            "[Train] | Epoch: 0029 / 0100 | Batch: 0009 / 0012 | Loss: 0.1833\n",
            "[Train] | Epoch: 0029 / 0100 | Batch: 0010 / 0012 | Loss: 0.1733\n",
            "[Train] | Epoch: 0029 / 0100 | Batch: 0011 / 0012 | Loss: 0.1970\n",
            "[Train] | Epoch: 0029 / 0100 | Batch: 0012 / 0012 | Loss: 0.1667\n",
            "[Validation] | Epoch: 0029 / 0100 | Batch: 0001 / 0002 | Loss: 0.2367\n",
            "[Validation] | Epoch: 0029 / 0100 | Batch: 0002 / 0002 | Loss: 0.1981\n",
            "[Epoch 0029] Training Avg Loss: 0.1781 | Validation Avg Loss: 0.2174\n",
            "[Train] | Epoch: 0030 / 0100 | Batch: 0001 / 0012 | Loss: 0.2024\n",
            "[Train] | Epoch: 0030 / 0100 | Batch: 0002 / 0012 | Loss: 0.1628\n",
            "[Train] | Epoch: 0030 / 0100 | Batch: 0003 / 0012 | Loss: 0.1718\n",
            "[Train] | Epoch: 0030 / 0100 | Batch: 0004 / 0012 | Loss: 0.1876\n",
            "[Train] | Epoch: 0030 / 0100 | Batch: 0005 / 0012 | Loss: 0.2224\n",
            "[Train] | Epoch: 0030 / 0100 | Batch: 0006 / 0012 | Loss: 0.1721\n",
            "[Train] | Epoch: 0030 / 0100 | Batch: 0007 / 0012 | Loss: 0.1524\n",
            "[Train] | Epoch: 0030 / 0100 | Batch: 0008 / 0012 | Loss: 0.1773\n",
            "[Train] | Epoch: 0030 / 0100 | Batch: 0009 / 0012 | Loss: 0.1945\n",
            "[Train] | Epoch: 0030 / 0100 | Batch: 0010 / 0012 | Loss: 0.1808\n",
            "[Train] | Epoch: 0030 / 0100 | Batch: 0011 / 0012 | Loss: 0.1812\n",
            "[Train] | Epoch: 0030 / 0100 | Batch: 0012 / 0012 | Loss: 0.1593\n",
            "[Validation] | Epoch: 0030 / 0100 | Batch: 0001 / 0002 | Loss: 0.2524\n",
            "[Validation] | Epoch: 0030 / 0100 | Batch: 0002 / 0002 | Loss: 0.1996\n",
            "[Epoch 0030] Training Avg Loss: 0.1804 | Validation Avg Loss: 0.2260\n",
            "[Train] | Epoch: 0031 / 0100 | Batch: 0001 / 0012 | Loss: 0.1789\n",
            "[Train] | Epoch: 0031 / 0100 | Batch: 0002 / 0012 | Loss: 0.1653\n",
            "[Train] | Epoch: 0031 / 0100 | Batch: 0003 / 0012 | Loss: 0.1873\n",
            "[Train] | Epoch: 0031 / 0100 | Batch: 0004 / 0012 | Loss: 0.1801\n",
            "[Train] | Epoch: 0031 / 0100 | Batch: 0005 / 0012 | Loss: 0.1812\n",
            "[Train] | Epoch: 0031 / 0100 | Batch: 0006 / 0012 | Loss: 0.2368\n",
            "[Train] | Epoch: 0031 / 0100 | Batch: 0007 / 0012 | Loss: 0.1795\n",
            "[Train] | Epoch: 0031 / 0100 | Batch: 0008 / 0012 | Loss: 0.1738\n",
            "[Train] | Epoch: 0031 / 0100 | Batch: 0009 / 0012 | Loss: 0.1964\n",
            "[Train] | Epoch: 0031 / 0100 | Batch: 0010 / 0012 | Loss: 0.1747\n",
            "[Train] | Epoch: 0031 / 0100 | Batch: 0011 / 0012 | Loss: 0.1721\n",
            "[Train] | Epoch: 0031 / 0100 | Batch: 0012 / 0012 | Loss: 0.1686\n",
            "[Validation] | Epoch: 0031 / 0100 | Batch: 0001 / 0002 | Loss: 0.2356\n",
            "[Validation] | Epoch: 0031 / 0100 | Batch: 0002 / 0002 | Loss: 0.2133\n",
            "[Epoch 0031] Training Avg Loss: 0.1829 | Validation Avg Loss: 0.2244\n",
            "[Train] | Epoch: 0032 / 0100 | Batch: 0001 / 0012 | Loss: 0.1793\n",
            "[Train] | Epoch: 0032 / 0100 | Batch: 0002 / 0012 | Loss: 0.1735\n",
            "[Train] | Epoch: 0032 / 0100 | Batch: 0003 / 0012 | Loss: 0.1835\n",
            "[Train] | Epoch: 0032 / 0100 | Batch: 0004 / 0012 | Loss: 0.1873\n",
            "[Train] | Epoch: 0032 / 0100 | Batch: 0005 / 0012 | Loss: 0.1721\n",
            "[Train] | Epoch: 0032 / 0100 | Batch: 0006 / 0012 | Loss: 0.1809\n",
            "[Train] | Epoch: 0032 / 0100 | Batch: 0007 / 0012 | Loss: 0.1608\n",
            "[Train] | Epoch: 0032 / 0100 | Batch: 0008 / 0012 | Loss: 0.2021\n",
            "[Train] | Epoch: 0032 / 0100 | Batch: 0009 / 0012 | Loss: 0.2134\n",
            "[Train] | Epoch: 0032 / 0100 | Batch: 0010 / 0012 | Loss: 0.1587\n",
            "[Train] | Epoch: 0032 / 0100 | Batch: 0011 / 0012 | Loss: 0.1706\n",
            "[Train] | Epoch: 0032 / 0100 | Batch: 0012 / 0012 | Loss: 0.1624\n",
            "[Validation] | Epoch: 0032 / 0100 | Batch: 0001 / 0002 | Loss: 0.2486\n",
            "[Validation] | Epoch: 0032 / 0100 | Batch: 0002 / 0002 | Loss: 0.2175\n",
            "[Epoch 0032] Training Avg Loss: 0.1787 | Validation Avg Loss: 0.2330\n",
            "[Train] | Epoch: 0033 / 0100 | Batch: 0001 / 0012 | Loss: 0.1531\n",
            "[Train] | Epoch: 0033 / 0100 | Batch: 0002 / 0012 | Loss: 0.1679\n",
            "[Train] | Epoch: 0033 / 0100 | Batch: 0003 / 0012 | Loss: 0.1938\n",
            "[Train] | Epoch: 0033 / 0100 | Batch: 0004 / 0012 | Loss: 0.1768\n",
            "[Train] | Epoch: 0033 / 0100 | Batch: 0005 / 0012 | Loss: 0.2446\n",
            "[Train] | Epoch: 0033 / 0100 | Batch: 0006 / 0012 | Loss: 0.1835\n",
            "[Train] | Epoch: 0033 / 0100 | Batch: 0007 / 0012 | Loss: 0.1609\n",
            "[Train] | Epoch: 0033 / 0100 | Batch: 0008 / 0012 | Loss: 0.1871\n",
            "[Train] | Epoch: 0033 / 0100 | Batch: 0009 / 0012 | Loss: 0.1954\n",
            "[Train] | Epoch: 0033 / 0100 | Batch: 0010 / 0012 | Loss: 0.1825\n",
            "[Train] | Epoch: 0033 / 0100 | Batch: 0011 / 0012 | Loss: 0.1744\n",
            "[Train] | Epoch: 0033 / 0100 | Batch: 0012 / 0012 | Loss: 0.1843\n",
            "[Validation] | Epoch: 0033 / 0100 | Batch: 0001 / 0002 | Loss: 0.2278\n",
            "[Validation] | Epoch: 0033 / 0100 | Batch: 0002 / 0002 | Loss: 0.2034\n",
            "[Epoch 0033] Training Avg Loss: 0.1837 | Validation Avg Loss: 0.2156\n",
            "[Train] | Epoch: 0034 / 0100 | Batch: 0001 / 0012 | Loss: 0.1660\n",
            "[Train] | Epoch: 0034 / 0100 | Batch: 0002 / 0012 | Loss: 0.1778\n",
            "[Train] | Epoch: 0034 / 0100 | Batch: 0003 / 0012 | Loss: 0.1822\n",
            "[Train] | Epoch: 0034 / 0100 | Batch: 0004 / 0012 | Loss: 0.1981\n",
            "[Train] | Epoch: 0034 / 0100 | Batch: 0005 / 0012 | Loss: 0.1647\n",
            "[Train] | Epoch: 0034 / 0100 | Batch: 0006 / 0012 | Loss: 0.1758\n",
            "[Train] | Epoch: 0034 / 0100 | Batch: 0007 / 0012 | Loss: 0.1888\n",
            "[Train] | Epoch: 0034 / 0100 | Batch: 0008 / 0012 | Loss: 0.1771\n",
            "[Train] | Epoch: 0034 / 0100 | Batch: 0009 / 0012 | Loss: 0.1610\n",
            "[Train] | Epoch: 0034 / 0100 | Batch: 0010 / 0012 | Loss: 0.1605\n",
            "[Train] | Epoch: 0034 / 0100 | Batch: 0011 / 0012 | Loss: 0.1942\n",
            "[Train] | Epoch: 0034 / 0100 | Batch: 0012 / 0012 | Loss: 0.1603\n",
            "[Validation] | Epoch: 0034 / 0100 | Batch: 0001 / 0002 | Loss: 0.2331\n",
            "[Validation] | Epoch: 0034 / 0100 | Batch: 0002 / 0002 | Loss: 0.2064\n",
            "[Epoch 0034] Training Avg Loss: 0.1755 | Validation Avg Loss: 0.2198\n",
            "[Train] | Epoch: 0035 / 0100 | Batch: 0001 / 0012 | Loss: 0.1594\n",
            "[Train] | Epoch: 0035 / 0100 | Batch: 0002 / 0012 | Loss: 0.1640\n",
            "[Train] | Epoch: 0035 / 0100 | Batch: 0003 / 0012 | Loss: 0.1782\n",
            "[Train] | Epoch: 0035 / 0100 | Batch: 0004 / 0012 | Loss: 0.1986\n",
            "[Train] | Epoch: 0035 / 0100 | Batch: 0005 / 0012 | Loss: 0.1587\n",
            "[Train] | Epoch: 0035 / 0100 | Batch: 0006 / 0012 | Loss: 0.1865\n",
            "[Train] | Epoch: 0035 / 0100 | Batch: 0007 / 0012 | Loss: 0.1728\n",
            "[Train] | Epoch: 0035 / 0100 | Batch: 0008 / 0012 | Loss: 0.1676\n",
            "[Train] | Epoch: 0035 / 0100 | Batch: 0009 / 0012 | Loss: 0.1790\n",
            "[Train] | Epoch: 0035 / 0100 | Batch: 0010 / 0012 | Loss: 0.1866\n",
            "[Train] | Epoch: 0035 / 0100 | Batch: 0011 / 0012 | Loss: 0.1747\n",
            "[Train] | Epoch: 0035 / 0100 | Batch: 0012 / 0012 | Loss: 0.1803\n",
            "[Validation] | Epoch: 0035 / 0100 | Batch: 0001 / 0002 | Loss: 0.2596\n",
            "[Validation] | Epoch: 0035 / 0100 | Batch: 0002 / 0002 | Loss: 0.1897\n",
            "[Epoch 0035] Training Avg Loss: 0.1755 | Validation Avg Loss: 0.2246\n",
            "[Train] | Epoch: 0036 / 0100 | Batch: 0001 / 0012 | Loss: 0.1629\n",
            "[Train] | Epoch: 0036 / 0100 | Batch: 0002 / 0012 | Loss: 0.1713\n",
            "[Train] | Epoch: 0036 / 0100 | Batch: 0003 / 0012 | Loss: 0.1648\n",
            "[Train] | Epoch: 0036 / 0100 | Batch: 0004 / 0012 | Loss: 0.1700\n",
            "[Train] | Epoch: 0036 / 0100 | Batch: 0005 / 0012 | Loss: 0.1794\n",
            "[Train] | Epoch: 0036 / 0100 | Batch: 0006 / 0012 | Loss: 0.2059\n",
            "[Train] | Epoch: 0036 / 0100 | Batch: 0007 / 0012 | Loss: 0.1716\n",
            "[Train] | Epoch: 0036 / 0100 | Batch: 0008 / 0012 | Loss: 0.1545\n",
            "[Train] | Epoch: 0036 / 0100 | Batch: 0009 / 0012 | Loss: 0.1931\n",
            "[Train] | Epoch: 0036 / 0100 | Batch: 0010 / 0012 | Loss: 0.1918\n",
            "[Train] | Epoch: 0036 / 0100 | Batch: 0011 / 0012 | Loss: 0.1705\n",
            "[Train] | Epoch: 0036 / 0100 | Batch: 0012 / 0012 | Loss: 0.1671\n",
            "[Validation] | Epoch: 0036 / 0100 | Batch: 0001 / 0002 | Loss: 0.2760\n",
            "[Validation] | Epoch: 0036 / 0100 | Batch: 0002 / 0002 | Loss: 0.2028\n",
            "[Epoch 0036] Training Avg Loss: 0.1752 | Validation Avg Loss: 0.2394\n",
            "[Train] | Epoch: 0037 / 0100 | Batch: 0001 / 0012 | Loss: 0.1774\n",
            "[Train] | Epoch: 0037 / 0100 | Batch: 0002 / 0012 | Loss: 0.1561\n",
            "[Train] | Epoch: 0037 / 0100 | Batch: 0003 / 0012 | Loss: 0.1560\n",
            "[Train] | Epoch: 0037 / 0100 | Batch: 0004 / 0012 | Loss: 0.1732\n",
            "[Train] | Epoch: 0037 / 0100 | Batch: 0005 / 0012 | Loss: 0.1727\n",
            "[Train] | Epoch: 0037 / 0100 | Batch: 0006 / 0012 | Loss: 0.1763\n",
            "[Train] | Epoch: 0037 / 0100 | Batch: 0007 / 0012 | Loss: 0.1995\n",
            "[Train] | Epoch: 0037 / 0100 | Batch: 0008 / 0012 | Loss: 0.1680\n",
            "[Train] | Epoch: 0037 / 0100 | Batch: 0009 / 0012 | Loss: 0.1614\n",
            "[Train] | Epoch: 0037 / 0100 | Batch: 0010 / 0012 | Loss: 0.1993\n",
            "[Train] | Epoch: 0037 / 0100 | Batch: 0011 / 0012 | Loss: 0.1996\n",
            "[Train] | Epoch: 0037 / 0100 | Batch: 0012 / 0012 | Loss: 0.1634\n",
            "[Validation] | Epoch: 0037 / 0100 | Batch: 0001 / 0002 | Loss: 0.2270\n",
            "[Validation] | Epoch: 0037 / 0100 | Batch: 0002 / 0002 | Loss: 0.1987\n",
            "[Epoch 0037] Training Avg Loss: 0.1752 | Validation Avg Loss: 0.2129\n",
            "[Train] | Epoch: 0038 / 0100 | Batch: 0001 / 0012 | Loss: 0.1762\n",
            "[Train] | Epoch: 0038 / 0100 | Batch: 0002 / 0012 | Loss: 0.2007\n",
            "[Train] | Epoch: 0038 / 0100 | Batch: 0003 / 0012 | Loss: 0.1972\n",
            "[Train] | Epoch: 0038 / 0100 | Batch: 0004 / 0012 | Loss: 0.1638\n",
            "[Train] | Epoch: 0038 / 0100 | Batch: 0005 / 0012 | Loss: 0.1717\n",
            "[Train] | Epoch: 0038 / 0100 | Batch: 0006 / 0012 | Loss: 0.1621\n",
            "[Train] | Epoch: 0038 / 0100 | Batch: 0007 / 0012 | Loss: 0.1630\n",
            "[Train] | Epoch: 0038 / 0100 | Batch: 0008 / 0012 | Loss: 0.1783\n",
            "[Train] | Epoch: 0038 / 0100 | Batch: 0009 / 0012 | Loss: 0.1660\n",
            "[Train] | Epoch: 0038 / 0100 | Batch: 0010 / 0012 | Loss: 0.1694\n",
            "[Train] | Epoch: 0038 / 0100 | Batch: 0011 / 0012 | Loss: 0.1569\n",
            "[Train] | Epoch: 0038 / 0100 | Batch: 0012 / 0012 | Loss: 0.1799\n",
            "[Validation] | Epoch: 0038 / 0100 | Batch: 0001 / 0002 | Loss: 0.2439\n",
            "[Validation] | Epoch: 0038 / 0100 | Batch: 0002 / 0002 | Loss: 0.1896\n",
            "[Epoch 0038] Training Avg Loss: 0.1738 | Validation Avg Loss: 0.2168\n",
            "[Train] | Epoch: 0039 / 0100 | Batch: 0001 / 0012 | Loss: 0.1496\n",
            "[Train] | Epoch: 0039 / 0100 | Batch: 0002 / 0012 | Loss: 0.1589\n",
            "[Train] | Epoch: 0039 / 0100 | Batch: 0003 / 0012 | Loss: 0.1597\n",
            "[Train] | Epoch: 0039 / 0100 | Batch: 0004 / 0012 | Loss: 0.1863\n",
            "[Train] | Epoch: 0039 / 0100 | Batch: 0005 / 0012 | Loss: 0.1900\n",
            "[Train] | Epoch: 0039 / 0100 | Batch: 0006 / 0012 | Loss: 0.1710\n",
            "[Train] | Epoch: 0039 / 0100 | Batch: 0007 / 0012 | Loss: 0.1604\n",
            "[Train] | Epoch: 0039 / 0100 | Batch: 0008 / 0012 | Loss: 0.1913\n",
            "[Train] | Epoch: 0039 / 0100 | Batch: 0009 / 0012 | Loss: 0.1808\n",
            "[Train] | Epoch: 0039 / 0100 | Batch: 0010 / 0012 | Loss: 0.1674\n",
            "[Train] | Epoch: 0039 / 0100 | Batch: 0011 / 0012 | Loss: 0.1583\n",
            "[Train] | Epoch: 0039 / 0100 | Batch: 0012 / 0012 | Loss: 0.1995\n",
            "[Validation] | Epoch: 0039 / 0100 | Batch: 0001 / 0002 | Loss: 0.2431\n",
            "[Validation] | Epoch: 0039 / 0100 | Batch: 0002 / 0002 | Loss: 0.1871\n",
            "[Epoch 0039] Training Avg Loss: 0.1728 | Validation Avg Loss: 0.2151\n",
            "[Train] | Epoch: 0040 / 0100 | Batch: 0001 / 0012 | Loss: 0.1527\n",
            "[Train] | Epoch: 0040 / 0100 | Batch: 0002 / 0012 | Loss: 0.1568\n",
            "[Train] | Epoch: 0040 / 0100 | Batch: 0003 / 0012 | Loss: 0.1667\n",
            "[Train] | Epoch: 0040 / 0100 | Batch: 0004 / 0012 | Loss: 0.1915\n",
            "[Train] | Epoch: 0040 / 0100 | Batch: 0005 / 0012 | Loss: 0.1689\n",
            "[Train] | Epoch: 0040 / 0100 | Batch: 0006 / 0012 | Loss: 0.1653\n",
            "[Train] | Epoch: 0040 / 0100 | Batch: 0007 / 0012 | Loss: 0.1675\n",
            "[Train] | Epoch: 0040 / 0100 | Batch: 0008 / 0012 | Loss: 0.1612\n",
            "[Train] | Epoch: 0040 / 0100 | Batch: 0009 / 0012 | Loss: 0.1740\n",
            "[Train] | Epoch: 0040 / 0100 | Batch: 0010 / 0012 | Loss: 0.1690\n",
            "[Train] | Epoch: 0040 / 0100 | Batch: 0011 / 0012 | Loss: 0.1814\n",
            "[Train] | Epoch: 0040 / 0100 | Batch: 0012 / 0012 | Loss: 0.1951\n",
            "[Validation] | Epoch: 0040 / 0100 | Batch: 0001 / 0002 | Loss: 0.2384\n",
            "[Validation] | Epoch: 0040 / 0100 | Batch: 0002 / 0002 | Loss: 0.1827\n",
            "[Epoch 0040] Training Avg Loss: 0.1708 | Validation Avg Loss: 0.2106\n",
            "[Train] | Epoch: 0041 / 0100 | Batch: 0001 / 0012 | Loss: 0.1807\n",
            "[Train] | Epoch: 0041 / 0100 | Batch: 0002 / 0012 | Loss: 0.1720\n",
            "[Train] | Epoch: 0041 / 0100 | Batch: 0003 / 0012 | Loss: 0.1591\n",
            "[Train] | Epoch: 0041 / 0100 | Batch: 0004 / 0012 | Loss: 0.1691\n",
            "[Train] | Epoch: 0041 / 0100 | Batch: 0005 / 0012 | Loss: 0.1652\n",
            "[Train] | Epoch: 0041 / 0100 | Batch: 0006 / 0012 | Loss: 0.1838\n",
            "[Train] | Epoch: 0041 / 0100 | Batch: 0007 / 0012 | Loss: 0.1889\n",
            "[Train] | Epoch: 0041 / 0100 | Batch: 0008 / 0012 | Loss: 0.1538\n",
            "[Train] | Epoch: 0041 / 0100 | Batch: 0009 / 0012 | Loss: 0.1692\n",
            "[Train] | Epoch: 0041 / 0100 | Batch: 0010 / 0012 | Loss: 0.1882\n",
            "[Train] | Epoch: 0041 / 0100 | Batch: 0011 / 0012 | Loss: 0.1479\n",
            "[Train] | Epoch: 0041 / 0100 | Batch: 0012 / 0012 | Loss: 0.1697\n",
            "[Validation] | Epoch: 0041 / 0100 | Batch: 0001 / 0002 | Loss: 0.2552\n",
            "[Validation] | Epoch: 0041 / 0100 | Batch: 0002 / 0002 | Loss: 0.1876\n",
            "[Epoch 0041] Training Avg Loss: 0.1706 | Validation Avg Loss: 0.2214\n",
            "[Train] | Epoch: 0042 / 0100 | Batch: 0001 / 0012 | Loss: 0.1765\n",
            "[Train] | Epoch: 0042 / 0100 | Batch: 0002 / 0012 | Loss: 0.1483\n",
            "[Train] | Epoch: 0042 / 0100 | Batch: 0003 / 0012 | Loss: 0.1472\n",
            "[Train] | Epoch: 0042 / 0100 | Batch: 0004 / 0012 | Loss: 0.1663\n",
            "[Train] | Epoch: 0042 / 0100 | Batch: 0005 / 0012 | Loss: 0.1523\n",
            "[Train] | Epoch: 0042 / 0100 | Batch: 0006 / 0012 | Loss: 0.2081\n",
            "[Train] | Epoch: 0042 / 0100 | Batch: 0007 / 0012 | Loss: 0.2056\n",
            "[Train] | Epoch: 0042 / 0100 | Batch: 0008 / 0012 | Loss: 0.1803\n",
            "[Train] | Epoch: 0042 / 0100 | Batch: 0009 / 0012 | Loss: 0.1751\n",
            "[Train] | Epoch: 0042 / 0100 | Batch: 0010 / 0012 | Loss: 0.1731\n",
            "[Train] | Epoch: 0042 / 0100 | Batch: 0011 / 0012 | Loss: 0.2019\n",
            "[Train] | Epoch: 0042 / 0100 | Batch: 0012 / 0012 | Loss: 0.1734\n",
            "[Validation] | Epoch: 0042 / 0100 | Batch: 0001 / 0002 | Loss: 0.2460\n",
            "[Validation] | Epoch: 0042 / 0100 | Batch: 0002 / 0002 | Loss: 0.2013\n",
            "[Epoch 0042] Training Avg Loss: 0.1757 | Validation Avg Loss: 0.2237\n",
            "[Train] | Epoch: 0043 / 0100 | Batch: 0001 / 0012 | Loss: 0.1658\n",
            "[Train] | Epoch: 0043 / 0100 | Batch: 0002 / 0012 | Loss: 0.1725\n",
            "[Train] | Epoch: 0043 / 0100 | Batch: 0003 / 0012 | Loss: 0.1918\n",
            "[Train] | Epoch: 0043 / 0100 | Batch: 0004 / 0012 | Loss: 0.1610\n",
            "[Train] | Epoch: 0043 / 0100 | Batch: 0005 / 0012 | Loss: 0.1689\n",
            "[Train] | Epoch: 0043 / 0100 | Batch: 0006 / 0012 | Loss: 0.2064\n",
            "[Train] | Epoch: 0043 / 0100 | Batch: 0007 / 0012 | Loss: 0.1811\n",
            "[Train] | Epoch: 0043 / 0100 | Batch: 0008 / 0012 | Loss: 0.1724\n",
            "[Train] | Epoch: 0043 / 0100 | Batch: 0009 / 0012 | Loss: 0.1538\n",
            "[Train] | Epoch: 0043 / 0100 | Batch: 0010 / 0012 | Loss: 0.1473\n",
            "[Train] | Epoch: 0043 / 0100 | Batch: 0011 / 0012 | Loss: 0.1607\n",
            "[Train] | Epoch: 0043 / 0100 | Batch: 0012 / 0012 | Loss: 0.1394\n",
            "[Validation] | Epoch: 0043 / 0100 | Batch: 0001 / 0002 | Loss: 0.3095\n",
            "[Validation] | Epoch: 0043 / 0100 | Batch: 0002 / 0002 | Loss: 0.2076\n",
            "[Epoch 0043] Training Avg Loss: 0.1684 | Validation Avg Loss: 0.2586\n",
            "[Train] | Epoch: 0044 / 0100 | Batch: 0001 / 0012 | Loss: 0.1898\n",
            "[Train] | Epoch: 0044 / 0100 | Batch: 0002 / 0012 | Loss: 0.2193\n",
            "[Train] | Epoch: 0044 / 0100 | Batch: 0003 / 0012 | Loss: 0.1751\n",
            "[Train] | Epoch: 0044 / 0100 | Batch: 0004 / 0012 | Loss: 0.1593\n",
            "[Train] | Epoch: 0044 / 0100 | Batch: 0005 / 0012 | Loss: 0.1726\n",
            "[Train] | Epoch: 0044 / 0100 | Batch: 0006 / 0012 | Loss: 0.1704\n",
            "[Train] | Epoch: 0044 / 0100 | Batch: 0007 / 0012 | Loss: 0.1682\n",
            "[Train] | Epoch: 0044 / 0100 | Batch: 0008 / 0012 | Loss: 0.1692\n",
            "[Train] | Epoch: 0044 / 0100 | Batch: 0009 / 0012 | Loss: 0.1778\n",
            "[Train] | Epoch: 0044 / 0100 | Batch: 0010 / 0012 | Loss: 0.1978\n",
            "[Train] | Epoch: 0044 / 0100 | Batch: 0011 / 0012 | Loss: 0.1524\n",
            "[Train] | Epoch: 0044 / 0100 | Batch: 0012 / 0012 | Loss: 0.1715\n",
            "[Validation] | Epoch: 0044 / 0100 | Batch: 0001 / 0002 | Loss: 0.2330\n",
            "[Validation] | Epoch: 0044 / 0100 | Batch: 0002 / 0002 | Loss: 0.2004\n",
            "[Epoch 0044] Training Avg Loss: 0.1769 | Validation Avg Loss: 0.2167\n",
            "[Train] | Epoch: 0045 / 0100 | Batch: 0001 / 0012 | Loss: 0.1616\n",
            "[Train] | Epoch: 0045 / 0100 | Batch: 0002 / 0012 | Loss: 0.1618\n",
            "[Train] | Epoch: 0045 / 0100 | Batch: 0003 / 0012 | Loss: 0.1651\n",
            "[Train] | Epoch: 0045 / 0100 | Batch: 0004 / 0012 | Loss: 0.1520\n",
            "[Train] | Epoch: 0045 / 0100 | Batch: 0005 / 0012 | Loss: 0.2036\n",
            "[Train] | Epoch: 0045 / 0100 | Batch: 0006 / 0012 | Loss: 0.1910\n",
            "[Train] | Epoch: 0045 / 0100 | Batch: 0007 / 0012 | Loss: 0.1466\n",
            "[Train] | Epoch: 0045 / 0100 | Batch: 0008 / 0012 | Loss: 0.1680\n",
            "[Train] | Epoch: 0045 / 0100 | Batch: 0009 / 0012 | Loss: 0.1694\n",
            "[Train] | Epoch: 0045 / 0100 | Batch: 0010 / 0012 | Loss: 0.1802\n",
            "[Train] | Epoch: 0045 / 0100 | Batch: 0011 / 0012 | Loss: 0.1841\n",
            "[Train] | Epoch: 0045 / 0100 | Batch: 0012 / 0012 | Loss: 0.1576\n",
            "[Validation] | Epoch: 0045 / 0100 | Batch: 0001 / 0002 | Loss: 0.2553\n",
            "[Validation] | Epoch: 0045 / 0100 | Batch: 0002 / 0002 | Loss: 0.1881\n",
            "[Epoch 0045] Training Avg Loss: 0.1701 | Validation Avg Loss: 0.2217\n",
            "[Train] | Epoch: 0046 / 0100 | Batch: 0001 / 0012 | Loss: 0.1589\n",
            "[Train] | Epoch: 0046 / 0100 | Batch: 0002 / 0012 | Loss: 0.1566\n",
            "[Train] | Epoch: 0046 / 0100 | Batch: 0003 / 0012 | Loss: 0.1648\n",
            "[Train] | Epoch: 0046 / 0100 | Batch: 0004 / 0012 | Loss: 0.1567\n",
            "[Train] | Epoch: 0046 / 0100 | Batch: 0005 / 0012 | Loss: 0.1707\n",
            "[Train] | Epoch: 0046 / 0100 | Batch: 0006 / 0012 | Loss: 0.1771\n",
            "[Train] | Epoch: 0046 / 0100 | Batch: 0007 / 0012 | Loss: 0.1644\n",
            "[Train] | Epoch: 0046 / 0100 | Batch: 0008 / 0012 | Loss: 0.1526\n",
            "[Train] | Epoch: 0046 / 0100 | Batch: 0009 / 0012 | Loss: 0.1778\n",
            "[Train] | Epoch: 0046 / 0100 | Batch: 0010 / 0012 | Loss: 0.1976\n",
            "[Train] | Epoch: 0046 / 0100 | Batch: 0011 / 0012 | Loss: 0.1663\n",
            "[Train] | Epoch: 0046 / 0100 | Batch: 0012 / 0012 | Loss: 0.1860\n",
            "[Validation] | Epoch: 0046 / 0100 | Batch: 0001 / 0002 | Loss: 0.2690\n",
            "[Validation] | Epoch: 0046 / 0100 | Batch: 0002 / 0002 | Loss: 0.2000\n",
            "[Epoch 0046] Training Avg Loss: 0.1691 | Validation Avg Loss: 0.2345\n",
            "[Train] | Epoch: 0047 / 0100 | Batch: 0001 / 0012 | Loss: 0.1509\n",
            "[Train] | Epoch: 0047 / 0100 | Batch: 0002 / 0012 | Loss: 0.1641\n",
            "[Train] | Epoch: 0047 / 0100 | Batch: 0003 / 0012 | Loss: 0.1612\n",
            "[Train] | Epoch: 0047 / 0100 | Batch: 0004 / 0012 | Loss: 0.1794\n",
            "[Train] | Epoch: 0047 / 0100 | Batch: 0005 / 0012 | Loss: 0.1967\n",
            "[Train] | Epoch: 0047 / 0100 | Batch: 0006 / 0012 | Loss: 0.1708\n",
            "[Train] | Epoch: 0047 / 0100 | Batch: 0007 / 0012 | Loss: 0.1691\n",
            "[Train] | Epoch: 0047 / 0100 | Batch: 0008 / 0012 | Loss: 0.1648\n",
            "[Train] | Epoch: 0047 / 0100 | Batch: 0009 / 0012 | Loss: 0.1908\n",
            "[Train] | Epoch: 0047 / 0100 | Batch: 0010 / 0012 | Loss: 0.1588\n",
            "[Train] | Epoch: 0047 / 0100 | Batch: 0011 / 0012 | Loss: 0.1915\n",
            "[Train] | Epoch: 0047 / 0100 | Batch: 0012 / 0012 | Loss: 0.1649\n",
            "[Validation] | Epoch: 0047 / 0100 | Batch: 0001 / 0002 | Loss: 0.2523\n",
            "[Validation] | Epoch: 0047 / 0100 | Batch: 0002 / 0002 | Loss: 0.1901\n",
            "[Epoch 0047] Training Avg Loss: 0.1719 | Validation Avg Loss: 0.2212\n",
            "[Train] | Epoch: 0048 / 0100 | Batch: 0001 / 0012 | Loss: 0.2091\n",
            "[Train] | Epoch: 0048 / 0100 | Batch: 0002 / 0012 | Loss: 0.1692\n",
            "[Train] | Epoch: 0048 / 0100 | Batch: 0003 / 0012 | Loss: 0.1614\n",
            "[Train] | Epoch: 0048 / 0100 | Batch: 0004 / 0012 | Loss: 0.1600\n",
            "[Train] | Epoch: 0048 / 0100 | Batch: 0005 / 0012 | Loss: 0.1680\n",
            "[Train] | Epoch: 0048 / 0100 | Batch: 0006 / 0012 | Loss: 0.1653\n",
            "[Train] | Epoch: 0048 / 0100 | Batch: 0007 / 0012 | Loss: 0.1674\n",
            "[Train] | Epoch: 0048 / 0100 | Batch: 0008 / 0012 | Loss: 0.1501\n",
            "[Train] | Epoch: 0048 / 0100 | Batch: 0009 / 0012 | Loss: 0.1606\n",
            "[Train] | Epoch: 0048 / 0100 | Batch: 0010 / 0012 | Loss: 0.1857\n",
            "[Train] | Epoch: 0048 / 0100 | Batch: 0011 / 0012 | Loss: 0.1863\n",
            "[Train] | Epoch: 0048 / 0100 | Batch: 0012 / 0012 | Loss: 0.1612\n",
            "[Validation] | Epoch: 0048 / 0100 | Batch: 0001 / 0002 | Loss: 0.2330\n",
            "[Validation] | Epoch: 0048 / 0100 | Batch: 0002 / 0002 | Loss: 0.1996\n",
            "[Epoch 0048] Training Avg Loss: 0.1704 | Validation Avg Loss: 0.2163\n",
            "[Train] | Epoch: 0049 / 0100 | Batch: 0001 / 0012 | Loss: 0.1465\n",
            "[Train] | Epoch: 0049 / 0100 | Batch: 0002 / 0012 | Loss: 0.1660\n",
            "[Train] | Epoch: 0049 / 0100 | Batch: 0003 / 0012 | Loss: 0.1598\n",
            "[Train] | Epoch: 0049 / 0100 | Batch: 0004 / 0012 | Loss: 0.1896\n",
            "[Train] | Epoch: 0049 / 0100 | Batch: 0005 / 0012 | Loss: 0.1848\n",
            "[Train] | Epoch: 0049 / 0100 | Batch: 0006 / 0012 | Loss: 0.1649\n",
            "[Train] | Epoch: 0049 / 0100 | Batch: 0007 / 0012 | Loss: 0.1674\n",
            "[Train] | Epoch: 0049 / 0100 | Batch: 0008 / 0012 | Loss: 0.1522\n",
            "[Train] | Epoch: 0049 / 0100 | Batch: 0009 / 0012 | Loss: 0.1721\n",
            "[Train] | Epoch: 0049 / 0100 | Batch: 0010 / 0012 | Loss: 0.1668\n",
            "[Train] | Epoch: 0049 / 0100 | Batch: 0011 / 0012 | Loss: 0.1718\n",
            "[Train] | Epoch: 0049 / 0100 | Batch: 0012 / 0012 | Loss: 0.1558\n",
            "[Validation] | Epoch: 0049 / 0100 | Batch: 0001 / 0002 | Loss: 0.2635\n",
            "[Validation] | Epoch: 0049 / 0100 | Batch: 0002 / 0002 | Loss: 0.2285\n",
            "[Epoch 0049] Training Avg Loss: 0.1665 | Validation Avg Loss: 0.2460\n",
            "[Train] | Epoch: 0050 / 0100 | Batch: 0001 / 0012 | Loss: 0.1602\n",
            "[Train] | Epoch: 0050 / 0100 | Batch: 0002 / 0012 | Loss: 0.1665\n",
            "[Train] | Epoch: 0050 / 0100 | Batch: 0003 / 0012 | Loss: 0.1734\n",
            "[Train] | Epoch: 0050 / 0100 | Batch: 0004 / 0012 | Loss: 0.1517\n",
            "[Train] | Epoch: 0050 / 0100 | Batch: 0005 / 0012 | Loss: 0.1529\n",
            "[Train] | Epoch: 0050 / 0100 | Batch: 0006 / 0012 | Loss: 0.1786\n",
            "[Train] | Epoch: 0050 / 0100 | Batch: 0007 / 0012 | Loss: 0.1836\n",
            "[Train] | Epoch: 0050 / 0100 | Batch: 0008 / 0012 | Loss: 0.1776\n",
            "[Train] | Epoch: 0050 / 0100 | Batch: 0009 / 0012 | Loss: 0.1695\n",
            "[Train] | Epoch: 0050 / 0100 | Batch: 0010 / 0012 | Loss: 0.1618\n",
            "[Train] | Epoch: 0050 / 0100 | Batch: 0011 / 0012 | Loss: 0.1640\n",
            "[Train] | Epoch: 0050 / 0100 | Batch: 0012 / 0012 | Loss: 0.1706\n",
            "[Validation] | Epoch: 0050 / 0100 | Batch: 0001 / 0002 | Loss: 0.2561\n",
            "[Validation] | Epoch: 0050 / 0100 | Batch: 0002 / 0002 | Loss: 0.1998\n",
            "[Epoch 0050] Training Avg Loss: 0.1675 | Validation Avg Loss: 0.2280\n",
            "[Train] | Epoch: 0051 / 0100 | Batch: 0001 / 0012 | Loss: 0.1579\n",
            "[Train] | Epoch: 0051 / 0100 | Batch: 0002 / 0012 | Loss: 0.1572\n",
            "[Train] | Epoch: 0051 / 0100 | Batch: 0003 / 0012 | Loss: 0.1682\n",
            "[Train] | Epoch: 0051 / 0100 | Batch: 0004 / 0012 | Loss: 0.2085\n",
            "[Train] | Epoch: 0051 / 0100 | Batch: 0005 / 0012 | Loss: 0.1503\n",
            "[Train] | Epoch: 0051 / 0100 | Batch: 0006 / 0012 | Loss: 0.1581\n",
            "[Train] | Epoch: 0051 / 0100 | Batch: 0007 / 0012 | Loss: 0.1574\n",
            "[Train] | Epoch: 0051 / 0100 | Batch: 0008 / 0012 | Loss: 0.1955\n",
            "[Train] | Epoch: 0051 / 0100 | Batch: 0009 / 0012 | Loss: 0.1557\n",
            "[Train] | Epoch: 0051 / 0100 | Batch: 0010 / 0012 | Loss: 0.1755\n",
            "[Train] | Epoch: 0051 / 0100 | Batch: 0011 / 0012 | Loss: 0.1756\n",
            "[Train] | Epoch: 0051 / 0100 | Batch: 0012 / 0012 | Loss: 0.1662\n",
            "[Validation] | Epoch: 0051 / 0100 | Batch: 0001 / 0002 | Loss: 0.2206\n",
            "[Validation] | Epoch: 0051 / 0100 | Batch: 0002 / 0002 | Loss: 0.2331\n",
            "[Epoch 0051] Training Avg Loss: 0.1688 | Validation Avg Loss: 0.2268\n",
            "[Train] | Epoch: 0052 / 0100 | Batch: 0001 / 0012 | Loss: 0.1771\n",
            "[Train] | Epoch: 0052 / 0100 | Batch: 0002 / 0012 | Loss: 0.1581\n",
            "[Train] | Epoch: 0052 / 0100 | Batch: 0003 / 0012 | Loss: 0.1665\n",
            "[Train] | Epoch: 0052 / 0100 | Batch: 0004 / 0012 | Loss: 0.1727\n",
            "[Train] | Epoch: 0052 / 0100 | Batch: 0005 / 0012 | Loss: 0.1430\n",
            "[Train] | Epoch: 0052 / 0100 | Batch: 0006 / 0012 | Loss: 0.1616\n",
            "[Train] | Epoch: 0052 / 0100 | Batch: 0007 / 0012 | Loss: 0.1715\n",
            "[Train] | Epoch: 0052 / 0100 | Batch: 0008 / 0012 | Loss: 0.1605\n",
            "[Train] | Epoch: 0052 / 0100 | Batch: 0009 / 0012 | Loss: 0.1619\n",
            "[Train] | Epoch: 0052 / 0100 | Batch: 0010 / 0012 | Loss: 0.1693\n",
            "[Train] | Epoch: 0052 / 0100 | Batch: 0011 / 0012 | Loss: 0.1504\n",
            "[Train] | Epoch: 0052 / 0100 | Batch: 0012 / 0012 | Loss: 0.1742\n",
            "[Validation] | Epoch: 0052 / 0100 | Batch: 0001 / 0002 | Loss: 0.2920\n",
            "[Validation] | Epoch: 0052 / 0100 | Batch: 0002 / 0002 | Loss: 0.1859\n",
            "[Epoch 0052] Training Avg Loss: 0.1639 | Validation Avg Loss: 0.2389\n",
            "[Train] | Epoch: 0053 / 0100 | Batch: 0001 / 0012 | Loss: 0.1642\n",
            "[Train] | Epoch: 0053 / 0100 | Batch: 0002 / 0012 | Loss: 0.1521\n",
            "[Train] | Epoch: 0053 / 0100 | Batch: 0003 / 0012 | Loss: 0.1681\n",
            "[Train] | Epoch: 0053 / 0100 | Batch: 0004 / 0012 | Loss: 0.1438\n",
            "[Train] | Epoch: 0053 / 0100 | Batch: 0005 / 0012 | Loss: 0.1619\n",
            "[Train] | Epoch: 0053 / 0100 | Batch: 0006 / 0012 | Loss: 0.1572\n",
            "[Train] | Epoch: 0053 / 0100 | Batch: 0007 / 0012 | Loss: 0.1490\n",
            "[Train] | Epoch: 0053 / 0100 | Batch: 0008 / 0012 | Loss: 0.1486\n",
            "[Train] | Epoch: 0053 / 0100 | Batch: 0009 / 0012 | Loss: 0.1438\n",
            "[Train] | Epoch: 0053 / 0100 | Batch: 0010 / 0012 | Loss: 0.1723\n",
            "[Train] | Epoch: 0053 / 0100 | Batch: 0011 / 0012 | Loss: 0.1875\n",
            "[Train] | Epoch: 0053 / 0100 | Batch: 0012 / 0012 | Loss: 0.1976\n",
            "[Validation] | Epoch: 0053 / 0100 | Batch: 0001 / 0002 | Loss: 0.2341\n",
            "[Validation] | Epoch: 0053 / 0100 | Batch: 0002 / 0002 | Loss: 0.2017\n",
            "[Epoch 0053] Training Avg Loss: 0.1622 | Validation Avg Loss: 0.2179\n",
            "[Train] | Epoch: 0054 / 0100 | Batch: 0001 / 0012 | Loss: 0.1772\n",
            "[Train] | Epoch: 0054 / 0100 | Batch: 0002 / 0012 | Loss: 0.1622\n",
            "[Train] | Epoch: 0054 / 0100 | Batch: 0003 / 0012 | Loss: 0.1659\n",
            "[Train] | Epoch: 0054 / 0100 | Batch: 0004 / 0012 | Loss: 0.1622\n",
            "[Train] | Epoch: 0054 / 0100 | Batch: 0005 / 0012 | Loss: 0.1824\n",
            "[Train] | Epoch: 0054 / 0100 | Batch: 0006 / 0012 | Loss: 0.1601\n",
            "[Train] | Epoch: 0054 / 0100 | Batch: 0007 / 0012 | Loss: 0.1555\n",
            "[Train] | Epoch: 0054 / 0100 | Batch: 0008 / 0012 | Loss: 0.1460\n",
            "[Train] | Epoch: 0054 / 0100 | Batch: 0009 / 0012 | Loss: 0.1610\n",
            "[Train] | Epoch: 0054 / 0100 | Batch: 0010 / 0012 | Loss: 0.1667\n",
            "[Train] | Epoch: 0054 / 0100 | Batch: 0011 / 0012 | Loss: 0.1668\n",
            "[Train] | Epoch: 0054 / 0100 | Batch: 0012 / 0012 | Loss: 0.1668\n",
            "[Validation] | Epoch: 0054 / 0100 | Batch: 0001 / 0002 | Loss: 0.2414\n",
            "[Validation] | Epoch: 0054 / 0100 | Batch: 0002 / 0002 | Loss: 0.1925\n",
            "[Epoch 0054] Training Avg Loss: 0.1644 | Validation Avg Loss: 0.2170\n",
            "[Train] | Epoch: 0055 / 0100 | Batch: 0001 / 0012 | Loss: 0.1758\n",
            "[Train] | Epoch: 0055 / 0100 | Batch: 0002 / 0012 | Loss: 0.1687\n",
            "[Train] | Epoch: 0055 / 0100 | Batch: 0003 / 0012 | Loss: 0.1543\n",
            "[Train] | Epoch: 0055 / 0100 | Batch: 0004 / 0012 | Loss: 0.1823\n",
            "[Train] | Epoch: 0055 / 0100 | Batch: 0005 / 0012 | Loss: 0.1811\n",
            "[Train] | Epoch: 0055 / 0100 | Batch: 0006 / 0012 | Loss: 0.1600\n",
            "[Train] | Epoch: 0055 / 0100 | Batch: 0007 / 0012 | Loss: 0.1720\n",
            "[Train] | Epoch: 0055 / 0100 | Batch: 0008 / 0012 | Loss: 0.1707\n",
            "[Train] | Epoch: 0055 / 0100 | Batch: 0009 / 0012 | Loss: 0.1607\n",
            "[Train] | Epoch: 0055 / 0100 | Batch: 0010 / 0012 | Loss: 0.1592\n",
            "[Train] | Epoch: 0055 / 0100 | Batch: 0011 / 0012 | Loss: 0.1735\n",
            "[Train] | Epoch: 0055 / 0100 | Batch: 0012 / 0012 | Loss: 0.1477\n",
            "[Validation] | Epoch: 0055 / 0100 | Batch: 0001 / 0002 | Loss: 0.2209\n",
            "[Validation] | Epoch: 0055 / 0100 | Batch: 0002 / 0002 | Loss: 0.1856\n",
            "[Epoch 0055] Training Avg Loss: 0.1672 | Validation Avg Loss: 0.2032\n",
            "[Train] | Epoch: 0056 / 0100 | Batch: 0001 / 0012 | Loss: 0.1614\n",
            "[Train] | Epoch: 0056 / 0100 | Batch: 0002 / 0012 | Loss: 0.1754\n",
            "[Train] | Epoch: 0056 / 0100 | Batch: 0003 / 0012 | Loss: 0.1542\n",
            "[Train] | Epoch: 0056 / 0100 | Batch: 0004 / 0012 | Loss: 0.1920\n",
            "[Train] | Epoch: 0056 / 0100 | Batch: 0005 / 0012 | Loss: 0.1764\n",
            "[Train] | Epoch: 0056 / 0100 | Batch: 0006 / 0012 | Loss: 0.1554\n",
            "[Train] | Epoch: 0056 / 0100 | Batch: 0007 / 0012 | Loss: 0.1610\n",
            "[Train] | Epoch: 0056 / 0100 | Batch: 0008 / 0012 | Loss: 0.1617\n",
            "[Train] | Epoch: 0056 / 0100 | Batch: 0009 / 0012 | Loss: 0.1601\n",
            "[Train] | Epoch: 0056 / 0100 | Batch: 0010 / 0012 | Loss: 0.1781\n",
            "[Train] | Epoch: 0056 / 0100 | Batch: 0011 / 0012 | Loss: 0.1468\n",
            "[Train] | Epoch: 0056 / 0100 | Batch: 0012 / 0012 | Loss: 0.1682\n",
            "[Validation] | Epoch: 0056 / 0100 | Batch: 0001 / 0002 | Loss: 0.2466\n",
            "[Validation] | Epoch: 0056 / 0100 | Batch: 0002 / 0002 | Loss: 0.1957\n",
            "[Epoch 0056] Training Avg Loss: 0.1659 | Validation Avg Loss: 0.2211\n",
            "[Train] | Epoch: 0057 / 0100 | Batch: 0001 / 0012 | Loss: 0.1464\n",
            "[Train] | Epoch: 0057 / 0100 | Batch: 0002 / 0012 | Loss: 0.1614\n",
            "[Train] | Epoch: 0057 / 0100 | Batch: 0003 / 0012 | Loss: 0.1953\n",
            "[Train] | Epoch: 0057 / 0100 | Batch: 0004 / 0012 | Loss: 0.1551\n",
            "[Train] | Epoch: 0057 / 0100 | Batch: 0005 / 0012 | Loss: 0.1534\n",
            "[Train] | Epoch: 0057 / 0100 | Batch: 0006 / 0012 | Loss: 0.1553\n",
            "[Train] | Epoch: 0057 / 0100 | Batch: 0007 / 0012 | Loss: 0.1893\n",
            "[Train] | Epoch: 0057 / 0100 | Batch: 0008 / 0012 | Loss: 0.1493\n",
            "[Train] | Epoch: 0057 / 0100 | Batch: 0009 / 0012 | Loss: 0.1549\n",
            "[Train] | Epoch: 0057 / 0100 | Batch: 0010 / 0012 | Loss: 0.1584\n",
            "[Train] | Epoch: 0057 / 0100 | Batch: 0011 / 0012 | Loss: 0.1664\n",
            "[Train] | Epoch: 0057 / 0100 | Batch: 0012 / 0012 | Loss: 0.1695\n",
            "[Validation] | Epoch: 0057 / 0100 | Batch: 0001 / 0002 | Loss: 0.2484\n",
            "[Validation] | Epoch: 0057 / 0100 | Batch: 0002 / 0002 | Loss: 0.1960\n",
            "[Epoch 0057] Training Avg Loss: 0.1629 | Validation Avg Loss: 0.2222\n",
            "[Train] | Epoch: 0058 / 0100 | Batch: 0001 / 0012 | Loss: 0.1465\n",
            "[Train] | Epoch: 0058 / 0100 | Batch: 0002 / 0012 | Loss: 0.1592\n",
            "[Train] | Epoch: 0058 / 0100 | Batch: 0003 / 0012 | Loss: 0.1562\n",
            "[Train] | Epoch: 0058 / 0100 | Batch: 0004 / 0012 | Loss: 0.1703\n",
            "[Train] | Epoch: 0058 / 0100 | Batch: 0005 / 0012 | Loss: 0.1633\n",
            "[Train] | Epoch: 0058 / 0100 | Batch: 0006 / 0012 | Loss: 0.1867\n",
            "[Train] | Epoch: 0058 / 0100 | Batch: 0007 / 0012 | Loss: 0.1430\n",
            "[Train] | Epoch: 0058 / 0100 | Batch: 0008 / 0012 | Loss: 0.1664\n",
            "[Train] | Epoch: 0058 / 0100 | Batch: 0009 / 0012 | Loss: 0.1707\n",
            "[Train] | Epoch: 0058 / 0100 | Batch: 0010 / 0012 | Loss: 0.1548\n",
            "[Train] | Epoch: 0058 / 0100 | Batch: 0011 / 0012 | Loss: 0.1539\n",
            "[Train] | Epoch: 0058 / 0100 | Batch: 0012 / 0012 | Loss: 0.1537\n",
            "[Validation] | Epoch: 0058 / 0100 | Batch: 0001 / 0002 | Loss: 0.2234\n",
            "[Validation] | Epoch: 0058 / 0100 | Batch: 0002 / 0002 | Loss: 0.1832\n",
            "[Epoch 0058] Training Avg Loss: 0.1604 | Validation Avg Loss: 0.2033\n",
            "[Train] | Epoch: 0059 / 0100 | Batch: 0001 / 0012 | Loss: 0.1981\n",
            "[Train] | Epoch: 0059 / 0100 | Batch: 0002 / 0012 | Loss: 0.1561\n",
            "[Train] | Epoch: 0059 / 0100 | Batch: 0003 / 0012 | Loss: 0.1479\n",
            "[Train] | Epoch: 0059 / 0100 | Batch: 0004 / 0012 | Loss: 0.1546\n",
            "[Train] | Epoch: 0059 / 0100 | Batch: 0005 / 0012 | Loss: 0.1529\n",
            "[Train] | Epoch: 0059 / 0100 | Batch: 0006 / 0012 | Loss: 0.1647\n",
            "[Train] | Epoch: 0059 / 0100 | Batch: 0007 / 0012 | Loss: 0.1482\n",
            "[Train] | Epoch: 0059 / 0100 | Batch: 0008 / 0012 | Loss: 0.1571\n",
            "[Train] | Epoch: 0059 / 0100 | Batch: 0009 / 0012 | Loss: 0.1629\n",
            "[Train] | Epoch: 0059 / 0100 | Batch: 0010 / 0012 | Loss: 0.1508\n",
            "[Train] | Epoch: 0059 / 0100 | Batch: 0011 / 0012 | Loss: 0.1740\n",
            "[Train] | Epoch: 0059 / 0100 | Batch: 0012 / 0012 | Loss: 0.1658\n",
            "[Validation] | Epoch: 0059 / 0100 | Batch: 0001 / 0002 | Loss: 0.2468\n",
            "[Validation] | Epoch: 0059 / 0100 | Batch: 0002 / 0002 | Loss: 0.2011\n",
            "[Epoch 0059] Training Avg Loss: 0.1611 | Validation Avg Loss: 0.2240\n",
            "[Train] | Epoch: 0060 / 0100 | Batch: 0001 / 0012 | Loss: 0.1827\n",
            "[Train] | Epoch: 0060 / 0100 | Batch: 0002 / 0012 | Loss: 0.1619\n",
            "[Train] | Epoch: 0060 / 0100 | Batch: 0003 / 0012 | Loss: 0.1505\n",
            "[Train] | Epoch: 0060 / 0100 | Batch: 0004 / 0012 | Loss: 0.1415\n",
            "[Train] | Epoch: 0060 / 0100 | Batch: 0005 / 0012 | Loss: 0.1577\n",
            "[Train] | Epoch: 0060 / 0100 | Batch: 0006 / 0012 | Loss: 0.1698\n",
            "[Train] | Epoch: 0060 / 0100 | Batch: 0007 / 0012 | Loss: 0.1535\n",
            "[Train] | Epoch: 0060 / 0100 | Batch: 0008 / 0012 | Loss: 0.1483\n",
            "[Train] | Epoch: 0060 / 0100 | Batch: 0009 / 0012 | Loss: 0.1680\n",
            "[Train] | Epoch: 0060 / 0100 | Batch: 0010 / 0012 | Loss: 0.1608\n",
            "[Train] | Epoch: 0060 / 0100 | Batch: 0011 / 0012 | Loss: 0.1637\n",
            "[Train] | Epoch: 0060 / 0100 | Batch: 0012 / 0012 | Loss: 0.1899\n",
            "[Validation] | Epoch: 0060 / 0100 | Batch: 0001 / 0002 | Loss: 0.2315\n",
            "[Validation] | Epoch: 0060 / 0100 | Batch: 0002 / 0002 | Loss: 0.1940\n",
            "[Epoch 0060] Training Avg Loss: 0.1623 | Validation Avg Loss: 0.2127\n",
            "[Train] | Epoch: 0061 / 0100 | Batch: 0001 / 0012 | Loss: 0.1515\n",
            "[Train] | Epoch: 0061 / 0100 | Batch: 0002 / 0012 | Loss: 0.1412\n",
            "[Train] | Epoch: 0061 / 0100 | Batch: 0003 / 0012 | Loss: 0.1599\n",
            "[Train] | Epoch: 0061 / 0100 | Batch: 0004 / 0012 | Loss: 0.1841\n",
            "[Train] | Epoch: 0061 / 0100 | Batch: 0005 / 0012 | Loss: 0.1550\n",
            "[Train] | Epoch: 0061 / 0100 | Batch: 0006 / 0012 | Loss: 0.1552\n",
            "[Train] | Epoch: 0061 / 0100 | Batch: 0007 / 0012 | Loss: 0.1629\n",
            "[Train] | Epoch: 0061 / 0100 | Batch: 0008 / 0012 | Loss: 0.1567\n",
            "[Train] | Epoch: 0061 / 0100 | Batch: 0009 / 0012 | Loss: 0.1696\n",
            "[Train] | Epoch: 0061 / 0100 | Batch: 0010 / 0012 | Loss: 0.1671\n",
            "[Train] | Epoch: 0061 / 0100 | Batch: 0011 / 0012 | Loss: 0.1448\n",
            "[Train] | Epoch: 0061 / 0100 | Batch: 0012 / 0012 | Loss: 0.1518\n",
            "[Validation] | Epoch: 0061 / 0100 | Batch: 0001 / 0002 | Loss: 0.2833\n",
            "[Validation] | Epoch: 0061 / 0100 | Batch: 0002 / 0002 | Loss: 0.1858\n",
            "[Epoch 0061] Training Avg Loss: 0.1583 | Validation Avg Loss: 0.2346\n",
            "[Train] | Epoch: 0062 / 0100 | Batch: 0001 / 0012 | Loss: 0.1502\n",
            "[Train] | Epoch: 0062 / 0100 | Batch: 0002 / 0012 | Loss: 0.2021\n",
            "[Train] | Epoch: 0062 / 0100 | Batch: 0003 / 0012 | Loss: 0.1567\n",
            "[Train] | Epoch: 0062 / 0100 | Batch: 0004 / 0012 | Loss: 0.1866\n",
            "[Train] | Epoch: 0062 / 0100 | Batch: 0005 / 0012 | Loss: 0.1451\n",
            "[Train] | Epoch: 0062 / 0100 | Batch: 0006 / 0012 | Loss: 0.1610\n",
            "[Train] | Epoch: 0062 / 0100 | Batch: 0007 / 0012 | Loss: 0.1617\n",
            "[Train] | Epoch: 0062 / 0100 | Batch: 0008 / 0012 | Loss: 0.1647\n",
            "[Train] | Epoch: 0062 / 0100 | Batch: 0009 / 0012 | Loss: 0.1527\n",
            "[Train] | Epoch: 0062 / 0100 | Batch: 0010 / 0012 | Loss: 0.1597\n",
            "[Train] | Epoch: 0062 / 0100 | Batch: 0011 / 0012 | Loss: 0.1517\n",
            "[Train] | Epoch: 0062 / 0100 | Batch: 0012 / 0012 | Loss: 0.1425\n",
            "[Validation] | Epoch: 0062 / 0100 | Batch: 0001 / 0002 | Loss: 0.2537\n",
            "[Validation] | Epoch: 0062 / 0100 | Batch: 0002 / 0002 | Loss: 0.1946\n",
            "[Epoch 0062] Training Avg Loss: 0.1612 | Validation Avg Loss: 0.2242\n",
            "[Train] | Epoch: 0063 / 0100 | Batch: 0001 / 0012 | Loss: 0.1499\n",
            "[Train] | Epoch: 0063 / 0100 | Batch: 0002 / 0012 | Loss: 0.1437\n",
            "[Train] | Epoch: 0063 / 0100 | Batch: 0003 / 0012 | Loss: 0.1852\n",
            "[Train] | Epoch: 0063 / 0100 | Batch: 0004 / 0012 | Loss: 0.1622\n",
            "[Train] | Epoch: 0063 / 0100 | Batch: 0005 / 0012 | Loss: 0.1386\n",
            "[Train] | Epoch: 0063 / 0100 | Batch: 0006 / 0012 | Loss: 0.1684\n",
            "[Train] | Epoch: 0063 / 0100 | Batch: 0007 / 0012 | Loss: 0.1635\n",
            "[Train] | Epoch: 0063 / 0100 | Batch: 0008 / 0012 | Loss: 0.1796\n",
            "[Train] | Epoch: 0063 / 0100 | Batch: 0009 / 0012 | Loss: 0.1653\n",
            "[Train] | Epoch: 0063 / 0100 | Batch: 0010 / 0012 | Loss: 0.1500\n",
            "[Train] | Epoch: 0063 / 0100 | Batch: 0011 / 0012 | Loss: 0.1495\n",
            "[Train] | Epoch: 0063 / 0100 | Batch: 0012 / 0012 | Loss: 0.1724\n",
            "[Validation] | Epoch: 0063 / 0100 | Batch: 0001 / 0002 | Loss: 0.2308\n",
            "[Validation] | Epoch: 0063 / 0100 | Batch: 0002 / 0002 | Loss: 0.1771\n",
            "[Epoch 0063] Training Avg Loss: 0.1607 | Validation Avg Loss: 0.2040\n",
            "[Train] | Epoch: 0064 / 0100 | Batch: 0001 / 0012 | Loss: 0.1497\n",
            "[Train] | Epoch: 0064 / 0100 | Batch: 0002 / 0012 | Loss: 0.1661\n",
            "[Train] | Epoch: 0064 / 0100 | Batch: 0003 / 0012 | Loss: 0.1551\n",
            "[Train] | Epoch: 0064 / 0100 | Batch: 0004 / 0012 | Loss: 0.1469\n",
            "[Train] | Epoch: 0064 / 0100 | Batch: 0005 / 0012 | Loss: 0.1582\n",
            "[Train] | Epoch: 0064 / 0100 | Batch: 0006 / 0012 | Loss: 0.1511\n",
            "[Train] | Epoch: 0064 / 0100 | Batch: 0007 / 0012 | Loss: 0.1791\n",
            "[Train] | Epoch: 0064 / 0100 | Batch: 0008 / 0012 | Loss: 0.1640\n",
            "[Train] | Epoch: 0064 / 0100 | Batch: 0009 / 0012 | Loss: 0.1499\n",
            "[Train] | Epoch: 0064 / 0100 | Batch: 0010 / 0012 | Loss: 0.1521\n",
            "[Train] | Epoch: 0064 / 0100 | Batch: 0011 / 0012 | Loss: 0.1476\n",
            "[Train] | Epoch: 0064 / 0100 | Batch: 0012 / 0012 | Loss: 0.1611\n",
            "[Validation] | Epoch: 0064 / 0100 | Batch: 0001 / 0002 | Loss: 0.2876\n",
            "[Validation] | Epoch: 0064 / 0100 | Batch: 0002 / 0002 | Loss: 0.1936\n",
            "[Epoch 0064] Training Avg Loss: 0.1567 | Validation Avg Loss: 0.2406\n",
            "[Train] | Epoch: 0065 / 0100 | Batch: 0001 / 0012 | Loss: 0.1493\n",
            "[Train] | Epoch: 0065 / 0100 | Batch: 0002 / 0012 | Loss: 0.1492\n",
            "[Train] | Epoch: 0065 / 0100 | Batch: 0003 / 0012 | Loss: 0.1722\n",
            "[Train] | Epoch: 0065 / 0100 | Batch: 0004 / 0012 | Loss: 0.1450\n",
            "[Train] | Epoch: 0065 / 0100 | Batch: 0005 / 0012 | Loss: 0.1643\n",
            "[Train] | Epoch: 0065 / 0100 | Batch: 0006 / 0012 | Loss: 0.1787\n",
            "[Train] | Epoch: 0065 / 0100 | Batch: 0007 / 0012 | Loss: 0.1624\n",
            "[Train] | Epoch: 0065 / 0100 | Batch: 0008 / 0012 | Loss: 0.1473\n",
            "[Train] | Epoch: 0065 / 0100 | Batch: 0009 / 0012 | Loss: 0.1513\n",
            "[Train] | Epoch: 0065 / 0100 | Batch: 0010 / 0012 | Loss: 0.1630\n",
            "[Train] | Epoch: 0065 / 0100 | Batch: 0011 / 0012 | Loss: 0.1743\n",
            "[Train] | Epoch: 0065 / 0100 | Batch: 0012 / 0012 | Loss: 0.1633\n",
            "[Validation] | Epoch: 0065 / 0100 | Batch: 0001 / 0002 | Loss: 0.2281\n",
            "[Validation] | Epoch: 0065 / 0100 | Batch: 0002 / 0002 | Loss: 0.1929\n",
            "[Epoch 0065] Training Avg Loss: 0.1600 | Validation Avg Loss: 0.2105\n",
            "[Train] | Epoch: 0066 / 0100 | Batch: 0001 / 0012 | Loss: 0.1541\n",
            "[Train] | Epoch: 0066 / 0100 | Batch: 0002 / 0012 | Loss: 0.1735\n",
            "[Train] | Epoch: 0066 / 0100 | Batch: 0003 / 0012 | Loss: 0.1706\n",
            "[Train] | Epoch: 0066 / 0100 | Batch: 0004 / 0012 | Loss: 0.1535\n",
            "[Train] | Epoch: 0066 / 0100 | Batch: 0005 / 0012 | Loss: 0.1413\n",
            "[Train] | Epoch: 0066 / 0100 | Batch: 0006 / 0012 | Loss: 0.1427\n",
            "[Train] | Epoch: 0066 / 0100 | Batch: 0007 / 0012 | Loss: 0.1574\n",
            "[Train] | Epoch: 0066 / 0100 | Batch: 0008 / 0012 | Loss: 0.1571\n",
            "[Train] | Epoch: 0066 / 0100 | Batch: 0009 / 0012 | Loss: 0.1605\n",
            "[Train] | Epoch: 0066 / 0100 | Batch: 0010 / 0012 | Loss: 0.2019\n",
            "[Train] | Epoch: 0066 / 0100 | Batch: 0011 / 0012 | Loss: 0.1421\n",
            "[Train] | Epoch: 0066 / 0100 | Batch: 0012 / 0012 | Loss: 0.1562\n",
            "[Validation] | Epoch: 0066 / 0100 | Batch: 0001 / 0002 | Loss: 0.2570\n",
            "[Validation] | Epoch: 0066 / 0100 | Batch: 0002 / 0002 | Loss: 0.1890\n",
            "[Epoch 0066] Training Avg Loss: 0.1592 | Validation Avg Loss: 0.2230\n",
            "[Train] | Epoch: 0067 / 0100 | Batch: 0001 / 0012 | Loss: 0.1898\n",
            "[Train] | Epoch: 0067 / 0100 | Batch: 0002 / 0012 | Loss: 0.1593\n",
            "[Train] | Epoch: 0067 / 0100 | Batch: 0003 / 0012 | Loss: 0.1609\n",
            "[Train] | Epoch: 0067 / 0100 | Batch: 0004 / 0012 | Loss: 0.1623\n",
            "[Train] | Epoch: 0067 / 0100 | Batch: 0005 / 0012 | Loss: 0.1377\n",
            "[Train] | Epoch: 0067 / 0100 | Batch: 0006 / 0012 | Loss: 0.1656\n",
            "[Train] | Epoch: 0067 / 0100 | Batch: 0007 / 0012 | Loss: 0.1588\n",
            "[Train] | Epoch: 0067 / 0100 | Batch: 0008 / 0012 | Loss: 0.1361\n",
            "[Train] | Epoch: 0067 / 0100 | Batch: 0009 / 0012 | Loss: 0.1609\n",
            "[Train] | Epoch: 0067 / 0100 | Batch: 0010 / 0012 | Loss: 0.1512\n",
            "[Train] | Epoch: 0067 / 0100 | Batch: 0011 / 0012 | Loss: 0.1526\n",
            "[Train] | Epoch: 0067 / 0100 | Batch: 0012 / 0012 | Loss: 0.1836\n",
            "[Validation] | Epoch: 0067 / 0100 | Batch: 0001 / 0002 | Loss: 0.2410\n",
            "[Validation] | Epoch: 0067 / 0100 | Batch: 0002 / 0002 | Loss: 0.2008\n",
            "[Epoch 0067] Training Avg Loss: 0.1599 | Validation Avg Loss: 0.2209\n",
            "[Train] | Epoch: 0068 / 0100 | Batch: 0001 / 0012 | Loss: 0.1436\n",
            "[Train] | Epoch: 0068 / 0100 | Batch: 0002 / 0012 | Loss: 0.1650\n",
            "[Train] | Epoch: 0068 / 0100 | Batch: 0003 / 0012 | Loss: 0.1447\n",
            "[Train] | Epoch: 0068 / 0100 | Batch: 0004 / 0012 | Loss: 0.1690\n",
            "[Train] | Epoch: 0068 / 0100 | Batch: 0005 / 0012 | Loss: 0.1556\n",
            "[Train] | Epoch: 0068 / 0100 | Batch: 0006 / 0012 | Loss: 0.1618\n",
            "[Train] | Epoch: 0068 / 0100 | Batch: 0007 / 0012 | Loss: 0.1466\n",
            "[Train] | Epoch: 0068 / 0100 | Batch: 0008 / 0012 | Loss: 0.1664\n",
            "[Train] | Epoch: 0068 / 0100 | Batch: 0009 / 0012 | Loss: 0.1512\n",
            "[Train] | Epoch: 0068 / 0100 | Batch: 0010 / 0012 | Loss: 0.1636\n",
            "[Train] | Epoch: 0068 / 0100 | Batch: 0011 / 0012 | Loss: 0.1557\n",
            "[Train] | Epoch: 0068 / 0100 | Batch: 0012 / 0012 | Loss: 0.1718\n",
            "[Validation] | Epoch: 0068 / 0100 | Batch: 0001 / 0002 | Loss: 0.2564\n",
            "[Validation] | Epoch: 0068 / 0100 | Batch: 0002 / 0002 | Loss: 0.2027\n",
            "[Epoch 0068] Training Avg Loss: 0.1579 | Validation Avg Loss: 0.2295\n",
            "[Train] | Epoch: 0069 / 0100 | Batch: 0001 / 0012 | Loss: 0.1476\n",
            "[Train] | Epoch: 0069 / 0100 | Batch: 0002 / 0012 | Loss: 0.1607\n",
            "[Train] | Epoch: 0069 / 0100 | Batch: 0003 / 0012 | Loss: 0.1592\n",
            "[Train] | Epoch: 0069 / 0100 | Batch: 0004 / 0012 | Loss: 0.1650\n",
            "[Train] | Epoch: 0069 / 0100 | Batch: 0005 / 0012 | Loss: 0.1550\n",
            "[Train] | Epoch: 0069 / 0100 | Batch: 0006 / 0012 | Loss: 0.1521\n",
            "[Train] | Epoch: 0069 / 0100 | Batch: 0007 / 0012 | Loss: 0.1571\n",
            "[Train] | Epoch: 0069 / 0100 | Batch: 0008 / 0012 | Loss: 0.1603\n",
            "[Train] | Epoch: 0069 / 0100 | Batch: 0009 / 0012 | Loss: 0.1833\n",
            "[Train] | Epoch: 0069 / 0100 | Batch: 0010 / 0012 | Loss: 0.1493\n",
            "[Train] | Epoch: 0069 / 0100 | Batch: 0011 / 0012 | Loss: 0.1575\n",
            "[Train] | Epoch: 0069 / 0100 | Batch: 0012 / 0012 | Loss: 0.1446\n",
            "[Validation] | Epoch: 0069 / 0100 | Batch: 0001 / 0002 | Loss: 0.2630\n",
            "[Validation] | Epoch: 0069 / 0100 | Batch: 0002 / 0002 | Loss: 0.1870\n",
            "[Epoch 0069] Training Avg Loss: 0.1576 | Validation Avg Loss: 0.2250\n",
            "[Train] | Epoch: 0070 / 0100 | Batch: 0001 / 0012 | Loss: 0.1428\n",
            "[Train] | Epoch: 0070 / 0100 | Batch: 0002 / 0012 | Loss: 0.1554\n",
            "[Train] | Epoch: 0070 / 0100 | Batch: 0003 / 0012 | Loss: 0.1746\n",
            "[Train] | Epoch: 0070 / 0100 | Batch: 0004 / 0012 | Loss: 0.1393\n",
            "[Train] | Epoch: 0070 / 0100 | Batch: 0005 / 0012 | Loss: 0.1463\n",
            "[Train] | Epoch: 0070 / 0100 | Batch: 0006 / 0012 | Loss: 0.1553\n",
            "[Train] | Epoch: 0070 / 0100 | Batch: 0007 / 0012 | Loss: 0.1851\n",
            "[Train] | Epoch: 0070 / 0100 | Batch: 0008 / 0012 | Loss: 0.1429\n",
            "[Train] | Epoch: 0070 / 0100 | Batch: 0009 / 0012 | Loss: 0.1611\n",
            "[Train] | Epoch: 0070 / 0100 | Batch: 0010 / 0012 | Loss: 0.1476\n",
            "[Train] | Epoch: 0070 / 0100 | Batch: 0011 / 0012 | Loss: 0.1499\n",
            "[Train] | Epoch: 0070 / 0100 | Batch: 0012 / 0012 | Loss: 0.1511\n",
            "[Validation] | Epoch: 0070 / 0100 | Batch: 0001 / 0002 | Loss: 0.2606\n",
            "[Validation] | Epoch: 0070 / 0100 | Batch: 0002 / 0002 | Loss: 0.1862\n",
            "[Epoch 0070] Training Avg Loss: 0.1543 | Validation Avg Loss: 0.2234\n",
            "[Train] | Epoch: 0071 / 0100 | Batch: 0001 / 0012 | Loss: 0.1614\n",
            "[Train] | Epoch: 0071 / 0100 | Batch: 0002 / 0012 | Loss: 0.1372\n",
            "[Train] | Epoch: 0071 / 0100 | Batch: 0003 / 0012 | Loss: 0.1508\n",
            "[Train] | Epoch: 0071 / 0100 | Batch: 0004 / 0012 | Loss: 0.1405\n",
            "[Train] | Epoch: 0071 / 0100 | Batch: 0005 / 0012 | Loss: 0.1364\n",
            "[Train] | Epoch: 0071 / 0100 | Batch: 0006 / 0012 | Loss: 0.1792\n",
            "[Train] | Epoch: 0071 / 0100 | Batch: 0007 / 0012 | Loss: 0.1514\n",
            "[Train] | Epoch: 0071 / 0100 | Batch: 0008 / 0012 | Loss: 0.1740\n",
            "[Train] | Epoch: 0071 / 0100 | Batch: 0009 / 0012 | Loss: 0.1551\n",
            "[Train] | Epoch: 0071 / 0100 | Batch: 0010 / 0012 | Loss: 0.1662\n",
            "[Train] | Epoch: 0071 / 0100 | Batch: 0011 / 0012 | Loss: 0.1680\n",
            "[Train] | Epoch: 0071 / 0100 | Batch: 0012 / 0012 | Loss: 0.1465\n",
            "[Validation] | Epoch: 0071 / 0100 | Batch: 0001 / 0002 | Loss: 0.2617\n",
            "[Validation] | Epoch: 0071 / 0100 | Batch: 0002 / 0002 | Loss: 0.2028\n",
            "[Epoch 0071] Training Avg Loss: 0.1555 | Validation Avg Loss: 0.2322\n",
            "[Train] | Epoch: 0072 / 0100 | Batch: 0001 / 0012 | Loss: 0.1636\n",
            "[Train] | Epoch: 0072 / 0100 | Batch: 0002 / 0012 | Loss: 0.1705\n",
            "[Train] | Epoch: 0072 / 0100 | Batch: 0003 / 0012 | Loss: 0.1596\n",
            "[Train] | Epoch: 0072 / 0100 | Batch: 0004 / 0012 | Loss: 0.1447\n",
            "[Train] | Epoch: 0072 / 0100 | Batch: 0005 / 0012 | Loss: 0.1613\n",
            "[Train] | Epoch: 0072 / 0100 | Batch: 0006 / 0012 | Loss: 0.1325\n",
            "[Train] | Epoch: 0072 / 0100 | Batch: 0007 / 0012 | Loss: 0.1490\n",
            "[Train] | Epoch: 0072 / 0100 | Batch: 0008 / 0012 | Loss: 0.1832\n",
            "[Train] | Epoch: 0072 / 0100 | Batch: 0009 / 0012 | Loss: 0.1756\n",
            "[Train] | Epoch: 0072 / 0100 | Batch: 0010 / 0012 | Loss: 0.1645\n",
            "[Train] | Epoch: 0072 / 0100 | Batch: 0011 / 0012 | Loss: 0.1535\n",
            "[Train] | Epoch: 0072 / 0100 | Batch: 0012 / 0012 | Loss: 0.1522\n",
            "[Validation] | Epoch: 0072 / 0100 | Batch: 0001 / 0002 | Loss: 0.2441\n",
            "[Validation] | Epoch: 0072 / 0100 | Batch: 0002 / 0002 | Loss: 0.1907\n",
            "[Epoch 0072] Training Avg Loss: 0.1592 | Validation Avg Loss: 0.2174\n",
            "[Train] | Epoch: 0073 / 0100 | Batch: 0001 / 0012 | Loss: 0.1513\n",
            "[Train] | Epoch: 0073 / 0100 | Batch: 0002 / 0012 | Loss: 0.1623\n",
            "[Train] | Epoch: 0073 / 0100 | Batch: 0003 / 0012 | Loss: 0.1588\n",
            "[Train] | Epoch: 0073 / 0100 | Batch: 0004 / 0012 | Loss: 0.1408\n",
            "[Train] | Epoch: 0073 / 0100 | Batch: 0005 / 0012 | Loss: 0.1588\n",
            "[Train] | Epoch: 0073 / 0100 | Batch: 0006 / 0012 | Loss: 0.1577\n",
            "[Train] | Epoch: 0073 / 0100 | Batch: 0007 / 0012 | Loss: 0.1551\n",
            "[Train] | Epoch: 0073 / 0100 | Batch: 0008 / 0012 | Loss: 0.1491\n",
            "[Train] | Epoch: 0073 / 0100 | Batch: 0009 / 0012 | Loss: 0.1577\n",
            "[Train] | Epoch: 0073 / 0100 | Batch: 0010 / 0012 | Loss: 0.1608\n",
            "[Train] | Epoch: 0073 / 0100 | Batch: 0011 / 0012 | Loss: 0.1553\n",
            "[Train] | Epoch: 0073 / 0100 | Batch: 0012 / 0012 | Loss: 0.1716\n",
            "[Validation] | Epoch: 0073 / 0100 | Batch: 0001 / 0002 | Loss: 0.2583\n",
            "[Validation] | Epoch: 0073 / 0100 | Batch: 0002 / 0002 | Loss: 0.1859\n",
            "[Epoch 0073] Training Avg Loss: 0.1566 | Validation Avg Loss: 0.2221\n",
            "[Train] | Epoch: 0074 / 0100 | Batch: 0001 / 0012 | Loss: 0.1443\n",
            "[Train] | Epoch: 0074 / 0100 | Batch: 0002 / 0012 | Loss: 0.1425\n",
            "[Train] | Epoch: 0074 / 0100 | Batch: 0003 / 0012 | Loss: 0.1595\n",
            "[Train] | Epoch: 0074 / 0100 | Batch: 0004 / 0012 | Loss: 0.1657\n",
            "[Train] | Epoch: 0074 / 0100 | Batch: 0005 / 0012 | Loss: 0.1703\n",
            "[Train] | Epoch: 0074 / 0100 | Batch: 0006 / 0012 | Loss: 0.1448\n",
            "[Train] | Epoch: 0074 / 0100 | Batch: 0007 / 0012 | Loss: 0.1555\n",
            "[Train] | Epoch: 0074 / 0100 | Batch: 0008 / 0012 | Loss: 0.1398\n",
            "[Train] | Epoch: 0074 / 0100 | Batch: 0009 / 0012 | Loss: 0.1519\n",
            "[Train] | Epoch: 0074 / 0100 | Batch: 0010 / 0012 | Loss: 0.1495\n",
            "[Train] | Epoch: 0074 / 0100 | Batch: 0011 / 0012 | Loss: 0.1489\n",
            "[Train] | Epoch: 0074 / 0100 | Batch: 0012 / 0012 | Loss: 0.1445\n",
            "[Validation] | Epoch: 0074 / 0100 | Batch: 0001 / 0002 | Loss: 0.2551\n",
            "[Validation] | Epoch: 0074 / 0100 | Batch: 0002 / 0002 | Loss: 0.1875\n",
            "[Epoch 0074] Training Avg Loss: 0.1514 | Validation Avg Loss: 0.2213\n",
            "[Train] | Epoch: 0075 / 0100 | Batch: 0001 / 0012 | Loss: 0.1406\n",
            "[Train] | Epoch: 0075 / 0100 | Batch: 0002 / 0012 | Loss: 0.1602\n",
            "[Train] | Epoch: 0075 / 0100 | Batch: 0003 / 0012 | Loss: 0.1771\n",
            "[Train] | Epoch: 0075 / 0100 | Batch: 0004 / 0012 | Loss: 0.1546\n",
            "[Train] | Epoch: 0075 / 0100 | Batch: 0005 / 0012 | Loss: 0.1435\n",
            "[Train] | Epoch: 0075 / 0100 | Batch: 0006 / 0012 | Loss: 0.1616\n",
            "[Train] | Epoch: 0075 / 0100 | Batch: 0007 / 0012 | Loss: 0.1475\n",
            "[Train] | Epoch: 0075 / 0100 | Batch: 0008 / 0012 | Loss: 0.1442\n",
            "[Train] | Epoch: 0075 / 0100 | Batch: 0009 / 0012 | Loss: 0.1727\n",
            "[Train] | Epoch: 0075 / 0100 | Batch: 0010 / 0012 | Loss: 0.1492\n",
            "[Train] | Epoch: 0075 / 0100 | Batch: 0011 / 0012 | Loss: 0.1659\n",
            "[Train] | Epoch: 0075 / 0100 | Batch: 0012 / 0012 | Loss: 0.1631\n",
            "[Validation] | Epoch: 0075 / 0100 | Batch: 0001 / 0002 | Loss: 0.2334\n",
            "[Validation] | Epoch: 0075 / 0100 | Batch: 0002 / 0002 | Loss: 0.1868\n",
            "[Epoch 0075] Training Avg Loss: 0.1567 | Validation Avg Loss: 0.2101\n",
            "[Train] | Epoch: 0076 / 0100 | Batch: 0001 / 0012 | Loss: 0.1687\n",
            "[Train] | Epoch: 0076 / 0100 | Batch: 0002 / 0012 | Loss: 0.1721\n",
            "[Train] | Epoch: 0076 / 0100 | Batch: 0003 / 0012 | Loss: 0.1617\n",
            "[Train] | Epoch: 0076 / 0100 | Batch: 0004 / 0012 | Loss: 0.1639\n",
            "[Train] | Epoch: 0076 / 0100 | Batch: 0005 / 0012 | Loss: 0.1732\n",
            "[Train] | Epoch: 0076 / 0100 | Batch: 0006 / 0012 | Loss: 0.1464\n",
            "[Train] | Epoch: 0076 / 0100 | Batch: 0007 / 0012 | Loss: 0.1474\n",
            "[Train] | Epoch: 0076 / 0100 | Batch: 0008 / 0012 | Loss: 0.1506\n",
            "[Train] | Epoch: 0076 / 0100 | Batch: 0009 / 0012 | Loss: 0.1431\n",
            "[Train] | Epoch: 0076 / 0100 | Batch: 0010 / 0012 | Loss: 0.1521\n",
            "[Train] | Epoch: 0076 / 0100 | Batch: 0011 / 0012 | Loss: 0.1516\n",
            "[Train] | Epoch: 0076 / 0100 | Batch: 0012 / 0012 | Loss: 0.1710\n",
            "[Validation] | Epoch: 0076 / 0100 | Batch: 0001 / 0002 | Loss: 0.2755\n",
            "[Validation] | Epoch: 0076 / 0100 | Batch: 0002 / 0002 | Loss: 0.1940\n",
            "[Epoch 0076] Training Avg Loss: 0.1585 | Validation Avg Loss: 0.2348\n",
            "[Train] | Epoch: 0077 / 0100 | Batch: 0001 / 0012 | Loss: 0.1611\n",
            "[Train] | Epoch: 0077 / 0100 | Batch: 0002 / 0012 | Loss: 0.1344\n",
            "[Train] | Epoch: 0077 / 0100 | Batch: 0003 / 0012 | Loss: 0.1555\n",
            "[Train] | Epoch: 0077 / 0100 | Batch: 0004 / 0012 | Loss: 0.1537\n",
            "[Train] | Epoch: 0077 / 0100 | Batch: 0005 / 0012 | Loss: 0.1688\n",
            "[Train] | Epoch: 0077 / 0100 | Batch: 0006 / 0012 | Loss: 0.1441\n",
            "[Train] | Epoch: 0077 / 0100 | Batch: 0007 / 0012 | Loss: 0.1417\n",
            "[Train] | Epoch: 0077 / 0100 | Batch: 0008 / 0012 | Loss: 0.1522\n",
            "[Train] | Epoch: 0077 / 0100 | Batch: 0009 / 0012 | Loss: 0.1499\n",
            "[Train] | Epoch: 0077 / 0100 | Batch: 0010 / 0012 | Loss: 0.1689\n",
            "[Train] | Epoch: 0077 / 0100 | Batch: 0011 / 0012 | Loss: 0.1512\n",
            "[Train] | Epoch: 0077 / 0100 | Batch: 0012 / 0012 | Loss: 0.1670\n",
            "[Validation] | Epoch: 0077 / 0100 | Batch: 0001 / 0002 | Loss: 0.2870\n",
            "[Validation] | Epoch: 0077 / 0100 | Batch: 0002 / 0002 | Loss: 0.1848\n",
            "[Epoch 0077] Training Avg Loss: 0.1540 | Validation Avg Loss: 0.2359\n",
            "[Train] | Epoch: 0078 / 0100 | Batch: 0001 / 0012 | Loss: 0.1555\n",
            "[Train] | Epoch: 0078 / 0100 | Batch: 0002 / 0012 | Loss: 0.1397\n",
            "[Train] | Epoch: 0078 / 0100 | Batch: 0003 / 0012 | Loss: 0.1598\n",
            "[Train] | Epoch: 0078 / 0100 | Batch: 0004 / 0012 | Loss: 0.1543\n",
            "[Train] | Epoch: 0078 / 0100 | Batch: 0005 / 0012 | Loss: 0.1347\n",
            "[Train] | Epoch: 0078 / 0100 | Batch: 0006 / 0012 | Loss: 0.1531\n",
            "[Train] | Epoch: 0078 / 0100 | Batch: 0007 / 0012 | Loss: 0.1540\n",
            "[Train] | Epoch: 0078 / 0100 | Batch: 0008 / 0012 | Loss: 0.1467\n",
            "[Train] | Epoch: 0078 / 0100 | Batch: 0009 / 0012 | Loss: 0.1355\n",
            "[Train] | Epoch: 0078 / 0100 | Batch: 0010 / 0012 | Loss: 0.1450\n",
            "[Train] | Epoch: 0078 / 0100 | Batch: 0011 / 0012 | Loss: 0.1567\n",
            "[Train] | Epoch: 0078 / 0100 | Batch: 0012 / 0012 | Loss: 0.1564\n",
            "[Validation] | Epoch: 0078 / 0100 | Batch: 0001 / 0002 | Loss: 0.2586\n",
            "[Validation] | Epoch: 0078 / 0100 | Batch: 0002 / 0002 | Loss: 0.2301\n",
            "[Epoch 0078] Training Avg Loss: 0.1493 | Validation Avg Loss: 0.2444\n",
            "[Train] | Epoch: 0079 / 0100 | Batch: 0001 / 0012 | Loss: 0.1696\n",
            "[Train] | Epoch: 0079 / 0100 | Batch: 0002 / 0012 | Loss: 0.1357\n",
            "[Train] | Epoch: 0079 / 0100 | Batch: 0003 / 0012 | Loss: 0.1561\n",
            "[Train] | Epoch: 0079 / 0100 | Batch: 0004 / 0012 | Loss: 0.1383\n",
            "[Train] | Epoch: 0079 / 0100 | Batch: 0005 / 0012 | Loss: 0.1415\n",
            "[Train] | Epoch: 0079 / 0100 | Batch: 0006 / 0012 | Loss: 0.1406\n",
            "[Train] | Epoch: 0079 / 0100 | Batch: 0007 / 0012 | Loss: 0.1817\n",
            "[Train] | Epoch: 0079 / 0100 | Batch: 0008 / 0012 | Loss: 0.1492\n",
            "[Train] | Epoch: 0079 / 0100 | Batch: 0009 / 0012 | Loss: 0.1512\n",
            "[Train] | Epoch: 0079 / 0100 | Batch: 0010 / 0012 | Loss: 0.1432\n",
            "[Train] | Epoch: 0079 / 0100 | Batch: 0011 / 0012 | Loss: 0.1683\n",
            "[Train] | Epoch: 0079 / 0100 | Batch: 0012 / 0012 | Loss: 0.1607\n",
            "[Validation] | Epoch: 0079 / 0100 | Batch: 0001 / 0002 | Loss: 0.2700\n",
            "[Validation] | Epoch: 0079 / 0100 | Batch: 0002 / 0002 | Loss: 0.1826\n",
            "[Epoch 0079] Training Avg Loss: 0.1530 | Validation Avg Loss: 0.2263\n",
            "[Train] | Epoch: 0080 / 0100 | Batch: 0001 / 0012 | Loss: 0.1309\n",
            "[Train] | Epoch: 0080 / 0100 | Batch: 0002 / 0012 | Loss: 0.1463\n",
            "[Train] | Epoch: 0080 / 0100 | Batch: 0003 / 0012 | Loss: 0.1347\n",
            "[Train] | Epoch: 0080 / 0100 | Batch: 0004 / 0012 | Loss: 0.1507\n",
            "[Train] | Epoch: 0080 / 0100 | Batch: 0005 / 0012 | Loss: 0.1463\n",
            "[Train] | Epoch: 0080 / 0100 | Batch: 0006 / 0012 | Loss: 0.1459\n",
            "[Train] | Epoch: 0080 / 0100 | Batch: 0007 / 0012 | Loss: 0.1497\n",
            "[Train] | Epoch: 0080 / 0100 | Batch: 0008 / 0012 | Loss: 0.1467\n",
            "[Train] | Epoch: 0080 / 0100 | Batch: 0009 / 0012 | Loss: 0.1941\n",
            "[Train] | Epoch: 0080 / 0100 | Batch: 0010 / 0012 | Loss: 0.1538\n",
            "[Train] | Epoch: 0080 / 0100 | Batch: 0011 / 0012 | Loss: 0.1697\n",
            "[Train] | Epoch: 0080 / 0100 | Batch: 0012 / 0012 | Loss: 0.1429\n",
            "[Validation] | Epoch: 0080 / 0100 | Batch: 0001 / 0002 | Loss: 0.2589\n",
            "[Validation] | Epoch: 0080 / 0100 | Batch: 0002 / 0002 | Loss: 0.2024\n",
            "[Epoch 0080] Training Avg Loss: 0.1510 | Validation Avg Loss: 0.2306\n",
            "[Train] | Epoch: 0081 / 0100 | Batch: 0001 / 0012 | Loss: 0.1736\n",
            "[Train] | Epoch: 0081 / 0100 | Batch: 0002 / 0012 | Loss: 0.1488\n",
            "[Train] | Epoch: 0081 / 0100 | Batch: 0003 / 0012 | Loss: 0.1519\n",
            "[Train] | Epoch: 0081 / 0100 | Batch: 0004 / 0012 | Loss: 0.1428\n",
            "[Train] | Epoch: 0081 / 0100 | Batch: 0005 / 0012 | Loss: 0.1610\n",
            "[Train] | Epoch: 0081 / 0100 | Batch: 0006 / 0012 | Loss: 0.1422\n",
            "[Train] | Epoch: 0081 / 0100 | Batch: 0007 / 0012 | Loss: 0.1392\n",
            "[Train] | Epoch: 0081 / 0100 | Batch: 0008 / 0012 | Loss: 0.1467\n",
            "[Train] | Epoch: 0081 / 0100 | Batch: 0009 / 0012 | Loss: 0.1644\n",
            "[Train] | Epoch: 0081 / 0100 | Batch: 0010 / 0012 | Loss: 0.1458\n",
            "[Train] | Epoch: 0081 / 0100 | Batch: 0011 / 0012 | Loss: 0.1652\n",
            "[Train] | Epoch: 0081 / 0100 | Batch: 0012 / 0012 | Loss: 0.1402\n",
            "[Validation] | Epoch: 0081 / 0100 | Batch: 0001 / 0002 | Loss: 0.2583\n",
            "[Validation] | Epoch: 0081 / 0100 | Batch: 0002 / 0002 | Loss: 0.2219\n",
            "[Epoch 0081] Training Avg Loss: 0.1518 | Validation Avg Loss: 0.2401\n",
            "[Train] | Epoch: 0082 / 0100 | Batch: 0001 / 0012 | Loss: 0.1400\n",
            "[Train] | Epoch: 0082 / 0100 | Batch: 0002 / 0012 | Loss: 0.1585\n",
            "[Train] | Epoch: 0082 / 0100 | Batch: 0003 / 0012 | Loss: 0.1384\n",
            "[Train] | Epoch: 0082 / 0100 | Batch: 0004 / 0012 | Loss: 0.1426\n",
            "[Train] | Epoch: 0082 / 0100 | Batch: 0005 / 0012 | Loss: 0.1480\n",
            "[Train] | Epoch: 0082 / 0100 | Batch: 0006 / 0012 | Loss: 0.1439\n",
            "[Train] | Epoch: 0082 / 0100 | Batch: 0007 / 0012 | Loss: 0.1517\n",
            "[Train] | Epoch: 0082 / 0100 | Batch: 0008 / 0012 | Loss: 0.1642\n",
            "[Train] | Epoch: 0082 / 0100 | Batch: 0009 / 0012 | Loss: 0.1495\n",
            "[Train] | Epoch: 0082 / 0100 | Batch: 0010 / 0012 | Loss: 0.1472\n",
            "[Train] | Epoch: 0082 / 0100 | Batch: 0011 / 0012 | Loss: 0.1663\n",
            "[Train] | Epoch: 0082 / 0100 | Batch: 0012 / 0012 | Loss: 0.1406\n",
            "[Validation] | Epoch: 0082 / 0100 | Batch: 0001 / 0002 | Loss: 0.2562\n",
            "[Validation] | Epoch: 0082 / 0100 | Batch: 0002 / 0002 | Loss: 0.1846\n",
            "[Epoch 0082] Training Avg Loss: 0.1492 | Validation Avg Loss: 0.2204\n",
            "[Train] | Epoch: 0083 / 0100 | Batch: 0001 / 0012 | Loss: 0.1273\n",
            "[Train] | Epoch: 0083 / 0100 | Batch: 0002 / 0012 | Loss: 0.1561\n",
            "[Train] | Epoch: 0083 / 0100 | Batch: 0003 / 0012 | Loss: 0.1578\n",
            "[Train] | Epoch: 0083 / 0100 | Batch: 0004 / 0012 | Loss: 0.1420\n",
            "[Train] | Epoch: 0083 / 0100 | Batch: 0005 / 0012 | Loss: 0.1893\n",
            "[Train] | Epoch: 0083 / 0100 | Batch: 0006 / 0012 | Loss: 0.1584\n",
            "[Train] | Epoch: 0083 / 0100 | Batch: 0007 / 0012 | Loss: 0.1569\n",
            "[Train] | Epoch: 0083 / 0100 | Batch: 0008 / 0012 | Loss: 0.1617\n",
            "[Train] | Epoch: 0083 / 0100 | Batch: 0009 / 0012 | Loss: 0.1490\n",
            "[Train] | Epoch: 0083 / 0100 | Batch: 0010 / 0012 | Loss: 0.1571\n",
            "[Train] | Epoch: 0083 / 0100 | Batch: 0011 / 0012 | Loss: 0.1418\n",
            "[Train] | Epoch: 0083 / 0100 | Batch: 0012 / 0012 | Loss: 0.1586\n",
            "[Validation] | Epoch: 0083 / 0100 | Batch: 0001 / 0002 | Loss: 0.2657\n",
            "[Validation] | Epoch: 0083 / 0100 | Batch: 0002 / 0002 | Loss: 0.2057\n",
            "[Epoch 0083] Training Avg Loss: 0.1547 | Validation Avg Loss: 0.2357\n",
            "[Train] | Epoch: 0084 / 0100 | Batch: 0001 / 0012 | Loss: 0.1621\n",
            "[Train] | Epoch: 0084 / 0100 | Batch: 0002 / 0012 | Loss: 0.1608\n",
            "[Train] | Epoch: 0084 / 0100 | Batch: 0003 / 0012 | Loss: 0.1556\n",
            "[Train] | Epoch: 0084 / 0100 | Batch: 0004 / 0012 | Loss: 0.1944\n",
            "[Train] | Epoch: 0084 / 0100 | Batch: 0005 / 0012 | Loss: 0.1368\n",
            "[Train] | Epoch: 0084 / 0100 | Batch: 0006 / 0012 | Loss: 0.1537\n",
            "[Train] | Epoch: 0084 / 0100 | Batch: 0007 / 0012 | Loss: 0.1395\n",
            "[Train] | Epoch: 0084 / 0100 | Batch: 0008 / 0012 | Loss: 0.1501\n",
            "[Train] | Epoch: 0084 / 0100 | Batch: 0009 / 0012 | Loss: 0.1537\n",
            "[Train] | Epoch: 0084 / 0100 | Batch: 0010 / 0012 | Loss: 0.1392\n",
            "[Train] | Epoch: 0084 / 0100 | Batch: 0011 / 0012 | Loss: 0.1557\n",
            "[Train] | Epoch: 0084 / 0100 | Batch: 0012 / 0012 | Loss: 0.1516\n",
            "[Validation] | Epoch: 0084 / 0100 | Batch: 0001 / 0002 | Loss: 0.2572\n",
            "[Validation] | Epoch: 0084 / 0100 | Batch: 0002 / 0002 | Loss: 0.1985\n",
            "[Epoch 0084] Training Avg Loss: 0.1544 | Validation Avg Loss: 0.2278\n",
            "[Train] | Epoch: 0085 / 0100 | Batch: 0001 / 0012 | Loss: 0.1703\n",
            "[Train] | Epoch: 0085 / 0100 | Batch: 0002 / 0012 | Loss: 0.1266\n",
            "[Train] | Epoch: 0085 / 0100 | Batch: 0003 / 0012 | Loss: 0.1588\n",
            "[Train] | Epoch: 0085 / 0100 | Batch: 0004 / 0012 | Loss: 0.1419\n",
            "[Train] | Epoch: 0085 / 0100 | Batch: 0005 / 0012 | Loss: 0.1320\n",
            "[Train] | Epoch: 0085 / 0100 | Batch: 0006 / 0012 | Loss: 0.1694\n",
            "[Train] | Epoch: 0085 / 0100 | Batch: 0007 / 0012 | Loss: 0.1507\n",
            "[Train] | Epoch: 0085 / 0100 | Batch: 0008 / 0012 | Loss: 0.1455\n",
            "[Train] | Epoch: 0085 / 0100 | Batch: 0009 / 0012 | Loss: 0.1385\n",
            "[Train] | Epoch: 0085 / 0100 | Batch: 0010 / 0012 | Loss: 0.1632\n",
            "[Train] | Epoch: 0085 / 0100 | Batch: 0011 / 0012 | Loss: 0.1562\n",
            "[Train] | Epoch: 0085 / 0100 | Batch: 0012 / 0012 | Loss: 0.1396\n",
            "[Validation] | Epoch: 0085 / 0100 | Batch: 0001 / 0002 | Loss: 0.2702\n",
            "[Validation] | Epoch: 0085 / 0100 | Batch: 0002 / 0002 | Loss: 0.1790\n",
            "[Epoch 0085] Training Avg Loss: 0.1494 | Validation Avg Loss: 0.2246\n",
            "[Train] | Epoch: 0086 / 0100 | Batch: 0001 / 0012 | Loss: 0.1580\n",
            "[Train] | Epoch: 0086 / 0100 | Batch: 0002 / 0012 | Loss: 0.1426\n",
            "[Train] | Epoch: 0086 / 0100 | Batch: 0003 / 0012 | Loss: 0.1556\n",
            "[Train] | Epoch: 0086 / 0100 | Batch: 0004 / 0012 | Loss: 0.1366\n",
            "[Train] | Epoch: 0086 / 0100 | Batch: 0005 / 0012 | Loss: 0.1732\n",
            "[Train] | Epoch: 0086 / 0100 | Batch: 0006 / 0012 | Loss: 0.1457\n",
            "[Train] | Epoch: 0086 / 0100 | Batch: 0007 / 0012 | Loss: 0.1360\n",
            "[Train] | Epoch: 0086 / 0100 | Batch: 0008 / 0012 | Loss: 0.1327\n",
            "[Train] | Epoch: 0086 / 0100 | Batch: 0009 / 0012 | Loss: 0.1400\n",
            "[Train] | Epoch: 0086 / 0100 | Batch: 0010 / 0012 | Loss: 0.1653\n",
            "[Train] | Epoch: 0086 / 0100 | Batch: 0011 / 0012 | Loss: 0.1825\n",
            "[Train] | Epoch: 0086 / 0100 | Batch: 0012 / 0012 | Loss: 0.1514\n",
            "[Validation] | Epoch: 0086 / 0100 | Batch: 0001 / 0002 | Loss: 0.2371\n",
            "[Validation] | Epoch: 0086 / 0100 | Batch: 0002 / 0002 | Loss: 0.1993\n",
            "[Epoch 0086] Training Avg Loss: 0.1516 | Validation Avg Loss: 0.2182\n",
            "[Train] | Epoch: 0087 / 0100 | Batch: 0001 / 0012 | Loss: 0.1359\n",
            "[Train] | Epoch: 0087 / 0100 | Batch: 0002 / 0012 | Loss: 0.1477\n",
            "[Train] | Epoch: 0087 / 0100 | Batch: 0003 / 0012 | Loss: 0.1641\n",
            "[Train] | Epoch: 0087 / 0100 | Batch: 0004 / 0012 | Loss: 0.1650\n",
            "[Train] | Epoch: 0087 / 0100 | Batch: 0005 / 0012 | Loss: 0.1513\n",
            "[Train] | Epoch: 0087 / 0100 | Batch: 0006 / 0012 | Loss: 0.1588\n",
            "[Train] | Epoch: 0087 / 0100 | Batch: 0007 / 0012 | Loss: 0.1416\n",
            "[Train] | Epoch: 0087 / 0100 | Batch: 0008 / 0012 | Loss: 0.1457\n",
            "[Train] | Epoch: 0087 / 0100 | Batch: 0009 / 0012 | Loss: 0.1339\n",
            "[Train] | Epoch: 0087 / 0100 | Batch: 0010 / 0012 | Loss: 0.1629\n",
            "[Train] | Epoch: 0087 / 0100 | Batch: 0011 / 0012 | Loss: 0.1584\n",
            "[Train] | Epoch: 0087 / 0100 | Batch: 0012 / 0012 | Loss: 0.1483\n",
            "[Validation] | Epoch: 0087 / 0100 | Batch: 0001 / 0002 | Loss: 0.2567\n",
            "[Validation] | Epoch: 0087 / 0100 | Batch: 0002 / 0002 | Loss: 0.1946\n",
            "[Epoch 0087] Training Avg Loss: 0.1511 | Validation Avg Loss: 0.2257\n",
            "[Train] | Epoch: 0088 / 0100 | Batch: 0001 / 0012 | Loss: 0.1453\n",
            "[Train] | Epoch: 0088 / 0100 | Batch: 0002 / 0012 | Loss: 0.1453\n",
            "[Train] | Epoch: 0088 / 0100 | Batch: 0003 / 0012 | Loss: 0.1563\n",
            "[Train] | Epoch: 0088 / 0100 | Batch: 0004 / 0012 | Loss: 0.1455\n",
            "[Train] | Epoch: 0088 / 0100 | Batch: 0005 / 0012 | Loss: 0.1481\n",
            "[Train] | Epoch: 0088 / 0100 | Batch: 0006 / 0012 | Loss: 0.1649\n",
            "[Train] | Epoch: 0088 / 0100 | Batch: 0007 / 0012 | Loss: 0.1299\n",
            "[Train] | Epoch: 0088 / 0100 | Batch: 0008 / 0012 | Loss: 0.1419\n",
            "[Train] | Epoch: 0088 / 0100 | Batch: 0009 / 0012 | Loss: 0.1430\n",
            "[Train] | Epoch: 0088 / 0100 | Batch: 0010 / 0012 | Loss: 0.1685\n",
            "[Train] | Epoch: 0088 / 0100 | Batch: 0011 / 0012 | Loss: 0.1469\n",
            "[Train] | Epoch: 0088 / 0100 | Batch: 0012 / 0012 | Loss: 0.1348\n",
            "[Validation] | Epoch: 0088 / 0100 | Batch: 0001 / 0002 | Loss: 0.2580\n",
            "[Validation] | Epoch: 0088 / 0100 | Batch: 0002 / 0002 | Loss: 0.1810\n",
            "[Epoch 0088] Training Avg Loss: 0.1475 | Validation Avg Loss: 0.2195\n",
            "[Train] | Epoch: 0089 / 0100 | Batch: 0001 / 0012 | Loss: 0.1449\n",
            "[Train] | Epoch: 0089 / 0100 | Batch: 0002 / 0012 | Loss: 0.1401\n",
            "[Train] | Epoch: 0089 / 0100 | Batch: 0003 / 0012 | Loss: 0.1537\n",
            "[Train] | Epoch: 0089 / 0100 | Batch: 0004 / 0012 | Loss: 0.1396\n",
            "[Train] | Epoch: 0089 / 0100 | Batch: 0005 / 0012 | Loss: 0.1533\n",
            "[Train] | Epoch: 0089 / 0100 | Batch: 0006 / 0012 | Loss: 0.1671\n",
            "[Train] | Epoch: 0089 / 0100 | Batch: 0007 / 0012 | Loss: 0.1376\n",
            "[Train] | Epoch: 0089 / 0100 | Batch: 0008 / 0012 | Loss: 0.1610\n",
            "[Train] | Epoch: 0089 / 0100 | Batch: 0009 / 0012 | Loss: 0.1638\n",
            "[Train] | Epoch: 0089 / 0100 | Batch: 0010 / 0012 | Loss: 0.1423\n",
            "[Train] | Epoch: 0089 / 0100 | Batch: 0011 / 0012 | Loss: 0.1387\n",
            "[Train] | Epoch: 0089 / 0100 | Batch: 0012 / 0012 | Loss: 0.1300\n",
            "[Validation] | Epoch: 0089 / 0100 | Batch: 0001 / 0002 | Loss: 0.2331\n",
            "[Validation] | Epoch: 0089 / 0100 | Batch: 0002 / 0002 | Loss: 0.1859\n",
            "[Epoch 0089] Training Avg Loss: 0.1477 | Validation Avg Loss: 0.2095\n",
            "[Train] | Epoch: 0090 / 0100 | Batch: 0001 / 0012 | Loss: 0.1568\n",
            "[Train] | Epoch: 0090 / 0100 | Batch: 0002 / 0012 | Loss: 0.1356\n",
            "[Train] | Epoch: 0090 / 0100 | Batch: 0003 / 0012 | Loss: 0.1436\n",
            "[Train] | Epoch: 0090 / 0100 | Batch: 0004 / 0012 | Loss: 0.1413\n",
            "[Train] | Epoch: 0090 / 0100 | Batch: 0005 / 0012 | Loss: 0.1636\n",
            "[Train] | Epoch: 0090 / 0100 | Batch: 0006 / 0012 | Loss: 0.1480\n",
            "[Train] | Epoch: 0090 / 0100 | Batch: 0007 / 0012 | Loss: 0.1448\n",
            "[Train] | Epoch: 0090 / 0100 | Batch: 0008 / 0012 | Loss: 0.1323\n",
            "[Train] | Epoch: 0090 / 0100 | Batch: 0009 / 0012 | Loss: 0.1568\n",
            "[Train] | Epoch: 0090 / 0100 | Batch: 0010 / 0012 | Loss: 0.1521\n",
            "[Train] | Epoch: 0090 / 0100 | Batch: 0011 / 0012 | Loss: 0.1443\n",
            "[Train] | Epoch: 0090 / 0100 | Batch: 0012 / 0012 | Loss: 0.1488\n",
            "[Validation] | Epoch: 0090 / 0100 | Batch: 0001 / 0002 | Loss: 0.2512\n",
            "[Validation] | Epoch: 0090 / 0100 | Batch: 0002 / 0002 | Loss: 0.1937\n",
            "[Epoch 0090] Training Avg Loss: 0.1473 | Validation Avg Loss: 0.2224\n",
            "[Train] | Epoch: 0091 / 0100 | Batch: 0001 / 0012 | Loss: 0.1552\n",
            "[Train] | Epoch: 0091 / 0100 | Batch: 0002 / 0012 | Loss: 0.1714\n",
            "[Train] | Epoch: 0091 / 0100 | Batch: 0003 / 0012 | Loss: 0.1317\n",
            "[Train] | Epoch: 0091 / 0100 | Batch: 0004 / 0012 | Loss: 0.1543\n",
            "[Train] | Epoch: 0091 / 0100 | Batch: 0005 / 0012 | Loss: 0.1387\n",
            "[Train] | Epoch: 0091 / 0100 | Batch: 0006 / 0012 | Loss: 0.1455\n",
            "[Train] | Epoch: 0091 / 0100 | Batch: 0007 / 0012 | Loss: 0.1428\n",
            "[Train] | Epoch: 0091 / 0100 | Batch: 0008 / 0012 | Loss: 0.1466\n",
            "[Train] | Epoch: 0091 / 0100 | Batch: 0009 / 0012 | Loss: 0.1406\n",
            "[Train] | Epoch: 0091 / 0100 | Batch: 0010 / 0012 | Loss: 0.1517\n",
            "[Train] | Epoch: 0091 / 0100 | Batch: 0011 / 0012 | Loss: 0.1358\n",
            "[Train] | Epoch: 0091 / 0100 | Batch: 0012 / 0012 | Loss: 0.1477\n",
            "[Validation] | Epoch: 0091 / 0100 | Batch: 0001 / 0002 | Loss: 0.2644\n",
            "[Validation] | Epoch: 0091 / 0100 | Batch: 0002 / 0002 | Loss: 0.1891\n",
            "[Epoch 0091] Training Avg Loss: 0.1468 | Validation Avg Loss: 0.2268\n",
            "[Train] | Epoch: 0092 / 0100 | Batch: 0001 / 0012 | Loss: 0.1350\n",
            "[Train] | Epoch: 0092 / 0100 | Batch: 0002 / 0012 | Loss: 0.1403\n",
            "[Train] | Epoch: 0092 / 0100 | Batch: 0003 / 0012 | Loss: 0.1411\n",
            "[Train] | Epoch: 0092 / 0100 | Batch: 0004 / 0012 | Loss: 0.1717\n",
            "[Train] | Epoch: 0092 / 0100 | Batch: 0005 / 0012 | Loss: 0.1436\n",
            "[Train] | Epoch: 0092 / 0100 | Batch: 0006 / 0012 | Loss: 0.1432\n",
            "[Train] | Epoch: 0092 / 0100 | Batch: 0007 / 0012 | Loss: 0.1428\n",
            "[Train] | Epoch: 0092 / 0100 | Batch: 0008 / 0012 | Loss: 0.1326\n",
            "[Train] | Epoch: 0092 / 0100 | Batch: 0009 / 0012 | Loss: 0.1363\n",
            "[Train] | Epoch: 0092 / 0100 | Batch: 0010 / 0012 | Loss: 0.1360\n",
            "[Train] | Epoch: 0092 / 0100 | Batch: 0011 / 0012 | Loss: 0.1406\n",
            "[Train] | Epoch: 0092 / 0100 | Batch: 0012 / 0012 | Loss: 0.1252\n",
            "[Validation] | Epoch: 0092 / 0100 | Batch: 0001 / 0002 | Loss: 0.2760\n",
            "[Validation] | Epoch: 0092 / 0100 | Batch: 0002 / 0002 | Loss: 0.2048\n",
            "[Epoch 0092] Training Avg Loss: 0.1407 | Validation Avg Loss: 0.2404\n",
            "[Train] | Epoch: 0093 / 0100 | Batch: 0001 / 0012 | Loss: 0.1791\n",
            "[Train] | Epoch: 0093 / 0100 | Batch: 0002 / 0012 | Loss: 0.1390\n",
            "[Train] | Epoch: 0093 / 0100 | Batch: 0003 / 0012 | Loss: 0.1642\n",
            "[Train] | Epoch: 0093 / 0100 | Batch: 0004 / 0012 | Loss: 0.1782\n",
            "[Train] | Epoch: 0093 / 0100 | Batch: 0005 / 0012 | Loss: 0.1496\n",
            "[Train] | Epoch: 0093 / 0100 | Batch: 0006 / 0012 | Loss: 0.1324\n",
            "[Train] | Epoch: 0093 / 0100 | Batch: 0007 / 0012 | Loss: 0.1668\n",
            "[Train] | Epoch: 0093 / 0100 | Batch: 0008 / 0012 | Loss: 0.1389\n",
            "[Train] | Epoch: 0093 / 0100 | Batch: 0009 / 0012 | Loss: 0.1428\n",
            "[Train] | Epoch: 0093 / 0100 | Batch: 0010 / 0012 | Loss: 0.1351\n",
            "[Train] | Epoch: 0093 / 0100 | Batch: 0011 / 0012 | Loss: 0.1294\n",
            "[Train] | Epoch: 0093 / 0100 | Batch: 0012 / 0012 | Loss: 0.1444\n",
            "[Validation] | Epoch: 0093 / 0100 | Batch: 0001 / 0002 | Loss: 0.2533\n",
            "[Validation] | Epoch: 0093 / 0100 | Batch: 0002 / 0002 | Loss: 0.2075\n",
            "[Epoch 0093] Training Avg Loss: 0.1500 | Validation Avg Loss: 0.2304\n",
            "[Train] | Epoch: 0094 / 0100 | Batch: 0001 / 0012 | Loss: 0.1509\n",
            "[Train] | Epoch: 0094 / 0100 | Batch: 0002 / 0012 | Loss: 0.1305\n",
            "[Train] | Epoch: 0094 / 0100 | Batch: 0003 / 0012 | Loss: 0.1478\n",
            "[Train] | Epoch: 0094 / 0100 | Batch: 0004 / 0012 | Loss: 0.1435\n",
            "[Train] | Epoch: 0094 / 0100 | Batch: 0005 / 0012 | Loss: 0.1489\n",
            "[Train] | Epoch: 0094 / 0100 | Batch: 0006 / 0012 | Loss: 0.1344\n",
            "[Train] | Epoch: 0094 / 0100 | Batch: 0007 / 0012 | Loss: 0.1434\n",
            "[Train] | Epoch: 0094 / 0100 | Batch: 0008 / 0012 | Loss: 0.1354\n",
            "[Train] | Epoch: 0094 / 0100 | Batch: 0009 / 0012 | Loss: 0.1774\n",
            "[Train] | Epoch: 0094 / 0100 | Batch: 0010 / 0012 | Loss: 0.1447\n",
            "[Train] | Epoch: 0094 / 0100 | Batch: 0011 / 0012 | Loss: 0.1404\n",
            "[Train] | Epoch: 0094 / 0100 | Batch: 0012 / 0012 | Loss: 0.1525\n",
            "[Validation] | Epoch: 0094 / 0100 | Batch: 0001 / 0002 | Loss: 0.2535\n",
            "[Validation] | Epoch: 0094 / 0100 | Batch: 0002 / 0002 | Loss: 0.1976\n",
            "[Epoch 0094] Training Avg Loss: 0.1458 | Validation Avg Loss: 0.2255\n",
            "[Train] | Epoch: 0095 / 0100 | Batch: 0001 / 0012 | Loss: 0.1436\n",
            "[Train] | Epoch: 0095 / 0100 | Batch: 0002 / 0012 | Loss: 0.1377\n",
            "[Train] | Epoch: 0095 / 0100 | Batch: 0003 / 0012 | Loss: 0.1344\n",
            "[Train] | Epoch: 0095 / 0100 | Batch: 0004 / 0012 | Loss: 0.1680\n",
            "[Train] | Epoch: 0095 / 0100 | Batch: 0005 / 0012 | Loss: 0.1409\n",
            "[Train] | Epoch: 0095 / 0100 | Batch: 0006 / 0012 | Loss: 0.1751\n",
            "[Train] | Epoch: 0095 / 0100 | Batch: 0007 / 0012 | Loss: 0.1498\n",
            "[Train] | Epoch: 0095 / 0100 | Batch: 0008 / 0012 | Loss: 0.1339\n",
            "[Train] | Epoch: 0095 / 0100 | Batch: 0009 / 0012 | Loss: 0.1410\n",
            "[Train] | Epoch: 0095 / 0100 | Batch: 0010 / 0012 | Loss: 0.1420\n",
            "[Train] | Epoch: 0095 / 0100 | Batch: 0011 / 0012 | Loss: 0.1532\n",
            "[Train] | Epoch: 0095 / 0100 | Batch: 0012 / 0012 | Loss: 0.1405\n",
            "[Validation] | Epoch: 0095 / 0100 | Batch: 0001 / 0002 | Loss: 0.2308\n",
            "[Validation] | Epoch: 0095 / 0100 | Batch: 0002 / 0002 | Loss: 0.1920\n",
            "[Epoch 0095] Training Avg Loss: 0.1467 | Validation Avg Loss: 0.2114\n",
            "[Train] | Epoch: 0096 / 0100 | Batch: 0001 / 0012 | Loss: 0.1705\n",
            "[Train] | Epoch: 0096 / 0100 | Batch: 0002 / 0012 | Loss: 0.1318\n",
            "[Train] | Epoch: 0096 / 0100 | Batch: 0003 / 0012 | Loss: 0.1440\n",
            "[Train] | Epoch: 0096 / 0100 | Batch: 0004 / 0012 | Loss: 0.1437\n",
            "[Train] | Epoch: 0096 / 0100 | Batch: 0005 / 0012 | Loss: 0.1189\n",
            "[Train] | Epoch: 0096 / 0100 | Batch: 0006 / 0012 | Loss: 0.1582\n",
            "[Train] | Epoch: 0096 / 0100 | Batch: 0007 / 0012 | Loss: 0.1399\n",
            "[Train] | Epoch: 0096 / 0100 | Batch: 0008 / 0012 | Loss: 0.1634\n",
            "[Train] | Epoch: 0096 / 0100 | Batch: 0009 / 0012 | Loss: 0.1426\n",
            "[Train] | Epoch: 0096 / 0100 | Batch: 0010 / 0012 | Loss: 0.1342\n",
            "[Train] | Epoch: 0096 / 0100 | Batch: 0011 / 0012 | Loss: 0.1533\n",
            "[Train] | Epoch: 0096 / 0100 | Batch: 0012 / 0012 | Loss: 0.1291\n",
            "[Validation] | Epoch: 0096 / 0100 | Batch: 0001 / 0002 | Loss: 0.2713\n",
            "[Validation] | Epoch: 0096 / 0100 | Batch: 0002 / 0002 | Loss: 0.1855\n",
            "[Epoch 0096] Training Avg Loss: 0.1441 | Validation Avg Loss: 0.2284\n",
            "[Train] | Epoch: 0097 / 0100 | Batch: 0001 / 0012 | Loss: 0.1601\n",
            "[Train] | Epoch: 0097 / 0100 | Batch: 0002 / 0012 | Loss: 0.1313\n",
            "[Train] | Epoch: 0097 / 0100 | Batch: 0003 / 0012 | Loss: 0.1430\n",
            "[Train] | Epoch: 0097 / 0100 | Batch: 0004 / 0012 | Loss: 0.1318\n",
            "[Train] | Epoch: 0097 / 0100 | Batch: 0005 / 0012 | Loss: 0.1502\n",
            "[Train] | Epoch: 0097 / 0100 | Batch: 0006 / 0012 | Loss: 0.1387\n",
            "[Train] | Epoch: 0097 / 0100 | Batch: 0007 / 0012 | Loss: 0.1441\n",
            "[Train] | Epoch: 0097 / 0100 | Batch: 0008 / 0012 | Loss: 0.1382\n",
            "[Train] | Epoch: 0097 / 0100 | Batch: 0009 / 0012 | Loss: 0.1498\n",
            "[Train] | Epoch: 0097 / 0100 | Batch: 0010 / 0012 | Loss: 0.1433\n",
            "[Train] | Epoch: 0097 / 0100 | Batch: 0011 / 0012 | Loss: 0.1314\n",
            "[Train] | Epoch: 0097 / 0100 | Batch: 0012 / 0012 | Loss: 0.1437\n",
            "[Validation] | Epoch: 0097 / 0100 | Batch: 0001 / 0002 | Loss: 0.2489\n",
            "[Validation] | Epoch: 0097 / 0100 | Batch: 0002 / 0002 | Loss: 0.1908\n",
            "[Epoch 0097] Training Avg Loss: 0.1421 | Validation Avg Loss: 0.2199\n",
            "[Train] | Epoch: 0098 / 0100 | Batch: 0001 / 0012 | Loss: 0.1297\n",
            "[Train] | Epoch: 0098 / 0100 | Batch: 0002 / 0012 | Loss: 0.1510\n",
            "[Train] | Epoch: 0098 / 0100 | Batch: 0003 / 0012 | Loss: 0.1341\n",
            "[Train] | Epoch: 0098 / 0100 | Batch: 0004 / 0012 | Loss: 0.1268\n",
            "[Train] | Epoch: 0098 / 0100 | Batch: 0005 / 0012 | Loss: 0.1501\n",
            "[Train] | Epoch: 0098 / 0100 | Batch: 0006 / 0012 | Loss: 0.1452\n",
            "[Train] | Epoch: 0098 / 0100 | Batch: 0007 / 0012 | Loss: 0.1459\n",
            "[Train] | Epoch: 0098 / 0100 | Batch: 0008 / 0012 | Loss: 0.1317\n",
            "[Train] | Epoch: 0098 / 0100 | Batch: 0009 / 0012 | Loss: 0.1379\n",
            "[Train] | Epoch: 0098 / 0100 | Batch: 0010 / 0012 | Loss: 0.1583\n",
            "[Train] | Epoch: 0098 / 0100 | Batch: 0011 / 0012 | Loss: 0.1588\n",
            "[Train] | Epoch: 0098 / 0100 | Batch: 0012 / 0012 | Loss: 0.1497\n",
            "[Validation] | Epoch: 0098 / 0100 | Batch: 0001 / 0002 | Loss: 0.2705\n",
            "[Validation] | Epoch: 0098 / 0100 | Batch: 0002 / 0002 | Loss: 0.1968\n",
            "[Epoch 0098] Training Avg Loss: 0.1433 | Validation Avg Loss: 0.2336\n",
            "[Train] | Epoch: 0099 / 0100 | Batch: 0001 / 0012 | Loss: 0.1438\n",
            "[Train] | Epoch: 0099 / 0100 | Batch: 0002 / 0012 | Loss: 0.1392\n",
            "[Train] | Epoch: 0099 / 0100 | Batch: 0003 / 0012 | Loss: 0.1327\n",
            "[Train] | Epoch: 0099 / 0100 | Batch: 0004 / 0012 | Loss: 0.1428\n",
            "[Train] | Epoch: 0099 / 0100 | Batch: 0005 / 0012 | Loss: 0.1563\n",
            "[Train] | Epoch: 0099 / 0100 | Batch: 0006 / 0012 | Loss: 0.1367\n",
            "[Train] | Epoch: 0099 / 0100 | Batch: 0007 / 0012 | Loss: 0.1388\n",
            "[Train] | Epoch: 0099 / 0100 | Batch: 0008 / 0012 | Loss: 0.1487\n",
            "[Train] | Epoch: 0099 / 0100 | Batch: 0009 / 0012 | Loss: 0.1309\n",
            "[Train] | Epoch: 0099 / 0100 | Batch: 0010 / 0012 | Loss: 0.1579\n",
            "[Train] | Epoch: 0099 / 0100 | Batch: 0011 / 0012 | Loss: 0.1552\n",
            "[Train] | Epoch: 0099 / 0100 | Batch: 0012 / 0012 | Loss: 0.1360\n",
            "[Validation] | Epoch: 0099 / 0100 | Batch: 0001 / 0002 | Loss: 0.2483\n",
            "[Validation] | Epoch: 0099 / 0100 | Batch: 0002 / 0002 | Loss: 0.1876\n",
            "[Epoch 0099] Training Avg Loss: 0.1433 | Validation Avg Loss: 0.2179\n",
            "[Train] | Epoch: 0100 / 0100 | Batch: 0001 / 0012 | Loss: 0.1372\n",
            "[Train] | Epoch: 0100 / 0100 | Batch: 0002 / 0012 | Loss: 0.1363\n",
            "[Train] | Epoch: 0100 / 0100 | Batch: 0003 / 0012 | Loss: 0.1336\n",
            "[Train] | Epoch: 0100 / 0100 | Batch: 0004 / 0012 | Loss: 0.1485\n",
            "[Train] | Epoch: 0100 / 0100 | Batch: 0005 / 0012 | Loss: 0.1444\n",
            "[Train] | Epoch: 0100 / 0100 | Batch: 0006 / 0012 | Loss: 0.1314\n",
            "[Train] | Epoch: 0100 / 0100 | Batch: 0007 / 0012 | Loss: 0.1349\n",
            "[Train] | Epoch: 0100 / 0100 | Batch: 0008 / 0012 | Loss: 0.1392\n",
            "[Train] | Epoch: 0100 / 0100 | Batch: 0009 / 0012 | Loss: 0.1375\n",
            "[Train] | Epoch: 0100 / 0100 | Batch: 0010 / 0012 | Loss: 0.1364\n",
            "[Train] | Epoch: 0100 / 0100 | Batch: 0011 / 0012 | Loss: 0.1444\n",
            "[Train] | Epoch: 0100 / 0100 | Batch: 0012 / 0012 | Loss: 0.1657\n",
            "[Validation] | Epoch: 0100 / 0100 | Batch: 0001 / 0002 | Loss: 0.2663\n",
            "[Validation] | Epoch: 0100 / 0100 | Batch: 0002 / 0002 | Loss: 0.1868\n",
            "[Epoch 0100] Training Avg Loss: 0.1408 | Validation Avg Loss: 0.2265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evalution**"
      ],
      "metadata": {
        "id": "Di2-7t1DALNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "# 1. Delete RandomFlip\n",
        "# 2. shuffle=False\n",
        "# 3. Tensorboard 사용 X\n",
        "# 4. Train X (Epoch 존재하지 않음)\n",
        "\n",
        "cfg = Config()\n",
        "transform = transforms.Compose([\n",
        "    GrayscaleNormalization(mean=0.5, std=0.5),\n",
        "    ToTensor(),\n",
        "])\n",
        "\n",
        "RESULTS_DIR = os.path.join(ROOT_DIR, 'test_results')\n",
        "if not os.path.exists(RESULTS_DIR):\n",
        "    os.makedirs(RESULTS_DIR)\n",
        "\n",
        "test_dataset = Dataset(imgs_dir=TEST_IMGS_DIR, labels_dir=TEST_LABELS_DIR, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=cfg.BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "test_data_num = len(test_dataset)\n",
        "test_batch_num = int(np.ceil(test_data_num / cfg.BATCH_SIZE)) # np.ceil 반올림\n",
        "\n",
        "# Network\n",
        "net = UNet().to(device)\n",
        "\n",
        "# Loss Function\n",
        "loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "# Optimizer\n",
        "optim = torch.optim.Adam(params=net.parameters(), lr=cfg.LEARNING_RATE)\n",
        "\n",
        "start_epoch = 0\n",
        "\n",
        "# Load Checkpoint File\n",
        "if os.listdir(CKPT_DIR):\n",
        "    net, optim, _ = load_net(ckpt_dir=CKPT_DIR, net=net, optim=optim)\n",
        "\n",
        "# Evaluation\n",
        "with torch.no_grad():\n",
        "    net.eval()  # Evaluation Mode\n",
        "    loss_arr = list()\n",
        "\n",
        "    for batch_idx, data in enumerate(test_loader, 1):\n",
        "        # Forward Propagation\n",
        "        img = data['img'].to(device)\n",
        "        label = data['label'].to(device)\n",
        "\n",
        "        output = net(img)\n",
        "\n",
        "        # Calc Loss Function\n",
        "        loss = loss_fn(output, label)\n",
        "        loss_arr.append(loss.item())\n",
        "\n",
        "        print_form = '[Test] | Batch: {:0>4d} / {:0>4d} | Loss: {:.4f}'\n",
        "        print(print_form.format(batch_idx, test_batch_num, loss_arr[-1]))\n",
        "\n",
        "        # Tensorboard\n",
        "        img = to_numpy(denormalization(img, mean=0.5, std=0.5))\n",
        "        label = to_numpy(label)\n",
        "        output = to_numpy(classify_class(output))\n",
        "\n",
        "        for j in range(label.shape[0]):\n",
        "            crt_id = int(test_batch_num * (batch_idx - 1) + j)\n",
        "\n",
        "            plt.imsave(os.path.join(RESULTS_DIR, f'img_{crt_id:04}.png'), img[j].squeeze(), cmap='gray')\n",
        "            plt.imsave(os.path.join(RESULTS_DIR, f'label_{crt_id:04}.png'), label[j].squeeze(), cmap='gray')\n",
        "            plt.imsave(os.path.join(RESULTS_DIR, f'output_{crt_id:04}.png'), output[j].squeeze(), cmap='gray')\n",
        "\n",
        "print_form = '[Result] | Avg Loss: {:0.4f}'\n",
        "print(print_form.format(np.mean(loss_arr)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sseDmC-TAK_1",
        "outputId": "340222f4-464c-40d4-9670-606af2a47019"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-eb2f52f3cd1a>:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_dict = torch.load(ckpt_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* Load /content/checkpoints/model_epoch0100.pth\n",
            "[Test] | Batch: 0001 / 0002 | Loss: 0.2564\n",
            "[Test] | Batch: 0002 / 0002 | Loss: 0.2443\n",
            "[Result] | Avg Loss: 0.2504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check the result from downloaded pics(in test _results folder)**"
      ],
      "metadata": {
        "id": "P3PTVqLISujQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pPvai6NdQyLp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}